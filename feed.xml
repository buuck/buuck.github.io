<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-05T18:08:00-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Micah Buuck, PhD</title><subtitle>The personal website of Micah Buuck. I'm a Research Associate at SLAC National Accelerator Laboratory, working at the intersection of physics and machine learning. Here you can find posts and write-ups of things I've worked on.</subtitle><author><name>Micah Buuck</name></author><entry><title type="html">Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept</title><link href="http://localhost:4000/blog/predictive-uncertainty-gammatpc/" rel="alternate" type="text/html" title="Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept" /><published>2022-11-05T00:00:00-07:00</published><updated>2022-11-05T00:00:00-07:00</updated><id>http://localhost:4000/blog/predictive-uncertainty-gammatpc</id><content type="html" xml:base="http://localhost:4000/blog/predictive-uncertainty-gammatpc/">&lt;h3 id=&quot;uncertainty-in-physics&quot;&gt;Uncertainty in Physics&lt;/h3&gt;

&lt;p&gt;Experimental physicists spend most of their research time doing 2 things: building their experiment, and trying to figure out how wrong their results could be. You’ll notice that I skipped right over the part where you actually do your experiment and analyze your data. It’s not that those things don’t happen, it’s just that trying to figure out what you don’t know or didn’t account for is usually a lot harder than doing the thing you set out to do! After all, a result or measurement doesn’t mean much if you don’t know how wrong it could be.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/muon-g-2-17-0188-20.hr_.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;The Muon G-2 experiment is located at Fermilab National Accelerator Laboratory in Batavia, Illinois. This image shows the muon storage ring that is the primary component of the experiment. You can see that it's pretty big. This image is taken from &lt;a href=&quot;https://news.fnal.gov/2019/03/muon-g-2-begins-second-run/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;their website&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Take the case of the &lt;a href=&quot;https://muon-g-2.fnal.gov/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Muon g-2&lt;/a&gt; experiment at Fermilab. This experiment was built make an extreeeemely precise measurement of something called the “anomalous magentic moment” of the muon&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. The reason physicists are interested in measuring this thing? In large part, it’s because the value that is predicted by particle theory does not agree with the number that has been measured with experiments. But it’s not just the fact that the numbers are different that is interesting, it’s the fact that experimentalists have been able to measure this number to incredible precision, and it’s waaay off from what theorists and their integrals say it should be. Uncertainty estimation (in this case, very small uncertainty estimation), is the linchpin of this experiment.&lt;/p&gt;

&lt;p&gt;Physicists are also very excited to use machine learning in their research. However, most machine learning algorithms do not include any way to estimate the uncertainty of their predictions. This is a huge problem for physicists if they want to use the output of a machine learning model to infer anything interesting. To be sure, there are use cases in physics for machine learning with no uncertainty estimation, but you probably can’t unlock the full potential of machine learning models if you have to constrain the use of them to the parts of an analysis where uncertainty quantification is not necessary.&lt;/p&gt;

&lt;h3 id=&quot;uncertainty-in-machine-learning&quot;&gt;Uncertainty in Machine Learning&lt;/h3&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/Tesla-Model-3-Autopilot-crash-highway-1422096078.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Self-driving cars are only as safe as their algorithms. This image is taken from this &lt;a href=&quot;https://electrek.co/2020/06/01/tesla-model-3-crashing-truck-autopilot-video-viral/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;electrek article&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A lack of uncertainty quantification in machine learning is also a huge problem for other people, with an obvious example being the people who are trying to create self-driving cars. If your car pulls up to an intersection and thinks there’s no stop sign, should it still stop? How sure is it that there’s no stop sign? Is it worse for a car to accidentally stop at an intersection where there is not actually a stop sign, or to blow through a stop sign at an intersection where there could be pedestrians expecting it to stop? If the decision-making algorithm the car is using can only say “there is a stop sign” or “there is not a stop sign”, then it’s always going to treat those two possible mistakes as equally bad, even though the car speeding though a stop sign is clearly much worse than stopping (or at least slowing down) at an intersection with no stop sign. Ideally you want the car to say to itself “I can’t really tell if there’s a stop sign here, so I’ll slow down and keep looking until I can get a better idea”. You may recognize this as what probably goes through your head in this situation as well. We’re all doing uncertainty quantification all the time!&lt;/p&gt;

&lt;p&gt;Being such an important topic, it’s unsurprising that Uncertainty Quantification (UQ) is a hot topic these days in machine learning research. There are a bunch of different approaches to quantify predictive uncertainty in your machine learning model, but I’m going to describe one that I have used in the past, and that I think is elegant and seems to work quite well! The rest of the sections in this post require some understanding of Bayesian statistics, so if you are not familiar with those words, or what a prior or posterior distribution are, check out this other post I wrote.&lt;/p&gt;

&lt;h3 id=&quot;flavors-of-uncertainty&quot;&gt;Flavors of Uncertainty&lt;/h3&gt;

&lt;p&gt;When physicists quantify their uncertainties, they usually break them up into two components: statistical and systematic. Statistical uncertainty refers to the fact that, because you only get to do one experiment to measure something (or at least a fixed number of them), you aren’t going to get the exact true value of the parameter you are trying to measure from your data. There’s a canonical example where you are trying to estimate the average height of a group of people by measuring their heights one at a time. Your best guess is going to be the average of the heights you have measured (assuming you are randomly sampling from the population of people), but the average height of the few people you’ve actually measured isn’t going to be the exact same as the average height of the whole group. Statistical uncertainty represents that difference. Systematic uncertainty is sort of a catch-all for all other types of possible sources of error, like: is the ruler you are using to measure people accurate? does the set of people you have measured have any systematic differences with the full population (i.e. you aren’t sampling truly randomly)? could the average height of the population be changing over time (e.g. lots of babies born)?&lt;/p&gt;

&lt;figure class=&quot;smaller&quot;&gt;
  
&lt;img src=&quot;/assets/images/Thomas_Bayes.gif&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Thomas Bayes would like you to quantify your knowledge please. This image is from his &lt;a href=&quot;https://en.wikipedia.org/wiki/Thomas_Bayes&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Wikipedia page&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In Machine Learning, uncertainty is also broken up into two components that are in many ways similar to the way it is done in physics. They are: aleatoric and epistemic. Aleatoric uncertainty is essentially a measure of the variance of your data. If the population whose average height you are trying to measure comes from all ages, that group will likely have a high level of aleatoric uncertainty, while if they are all the same age and sex, the aleatoric uncertainty will probably be lower. Epistemic uncertainty refers to uncertainty in the model itself, i.e. how confident a model is in its own predictions. Estimating the average height of a population does not require a complex model, but if you are trying to do something more complicated, like classifying pictures of animals by species, you will probably need a much more complex model that will have some degree of uncertainty when it makes predictions. In many situations, as described above, having access to this source of uncertainty can be very useful.&lt;/p&gt;

&lt;h3 id=&quot;gammatpc-and-evidential-deep-learning&quot;&gt;GammaTPC and Evidential Deep Learning&lt;/h3&gt;

&lt;p&gt;I’ve been working on a project to design a space-based telescope to look for gamma-rays of a particular type that have been a bit neglected for the last few decades. This telescope will contain a bunch of liquid argon, where the gamma-rays will scatter off of electrons in the argon. When they scatter, the hit electron starts flying through the argon, leaving behind a trail of destruction (i.e. other ionized electrons) before it eventually runs out of energy and stops. If we can precisely measure the initial locations of several scatters, we can use physics and math to reconstruct where the gamma-ray must have come from. Because these scattered electrons are leaving a cloud/track of ionized electrons behind, we need to be able to determine where the beginning of the track is to successfully make this measurement. This telescope will be able to produce a 3D-grid reconstruction of the electron track, so it’s plausible that a 3D convolutional neural network will be able to do a good job in reconstructing the location of the initial scatter.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/event_v2.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Left: A gamma-ray will scatter multiple times in the detector, and if you can figure out the order and locations, you can tell which direction it came from. Right: An example electron track (line), and the pixelated readout that the detector actually sees (circles). The circles are sized proportionally to the amount of charge measured at that location.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Precisely measuring the position of these electron scatters is key, but obtaining an uncertainty estimate for the observations is also very useful. If we know that a particular electron scatter is poorly measured, for whatever reason, it can be better to just completely ignore that data point, rather than include it in our data set knowing that it may be throwing off our measurements. I and several collaborators at SLAC have a &lt;a href=&quot;https://arxiv.org/abs/2207.07805&quot;&gt;paper&lt;/a&gt; out where we did just that. You can also find Jupyter notebooks to recreate the analysis &lt;a href=&quot;https://gitlab.com/probabilistic-uncertainty-for-gammatpc&quot;&gt;here&lt;/a&gt;. We tested several different kinds of uncertainty estimating models on our problem, and determined that an approach from 2020, called Evidential Deep Learning, worked the best for us. Evidential Deep Learning works well because it makes use of the handy mathematical relationships between conjugate distributions that I discuss in &lt;a href=&quot;/blog/bayes-theorem/&quot;&gt;another post&lt;/a&gt; to give you an uncertainty estimate without having to run the model a bunch of times (like an ensemble-based approach), or use a Bayesian neural network which can be much more computationally expensive. We also found that it performed better than other options in some key statistical metrics.&lt;/p&gt;

&lt;p&gt;We are trying to predict the location of an electron scatter from a 3D grid showing the track that it left as it traveled through its argon medium. Minimally, this requires a model that produces 3 real numbers as output: predictions for the x, y, and z components of the location. These can be compared to the true simulated locations, and if the loss function (i.e. likelihood) being used is the mean-squared-error, that comparison should follow a normal distribution. In Evidential Deep Learning, we no longer try to just predict 3 numbers, but instead we actually try to predict 12 numbers, 4 for each cardinal direction prediction. These numbers are the parameters characterizing a normal-inverse-gamma distribution, which is the cojugate prior for a normal likelihood with an unknown mean and unknown variance (i.e. our likelihood). In this case, the normal likelihood in some sense represents the distribution of our data set, with the mean being predictions for the electron track head locations, and the variance being the aleatoric uncertainty. The normal-inverse-gamma posterior then is effectively the epistemic uncertainty the model has for both its predictions and its estimate of the aleatoric uncertainty. When we train our model to predict all of the parameters for the posterior distribution (called hyperparameters), we are essentially training it to estimate a range of plausible predictions for each electron track, rather than just a single point estimate. After the model is trained, we can feed in new examples of electron tracks, and then use the predicted normal-inverse-gamma hyperparameters to get a prediction interval.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/NIG_distribution.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;A normal-inverse-gamma distribution is a joint probability distribution, which is the product of two unidimensional distributions. It has 2 random variables, and so its density function is 2D.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To find the heads of the electron tracks, we designed a multi-layer 3D convolutional neural network with 3 convolutional layers, 2 max pooling layers, and 1 global average pooling layer, followed by a feed-forward layer producing the final predictions. The model shown in the figure below is for a deterministic model we created as a benchmark to measure the Evidential model against. You can tell this because it ends with a length 3 vector for predicting the $X$, $Y$, and $Z$ components of the location. A very handy feature of Evidential Deep Learning is that it only requires swapping out the final layer for one that predicts 4x the number of outputs, and then altering the loss function appropriately. Furthermore, the author of the original paper has written a &lt;a href=&quot;https://github.com/aamini/evidential-deep-learning&quot;&gt;Python package&lt;/a&gt; that has code that can essentially be dropped into your existing setup. Instead of using the mean-squared-error for a loss function, you use the negative-log-likelihood (NLL) of the normal-inverse-gamma distribution, since minimizing the NLL produces the same optimized parameters as maximizing the likelihood does. And maximizing the likelihood while training your neural network essentially means that you are maximizing the ability of your network to predict the correct outputs from a given set of inputs.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/3DCNNDiagram_Alt.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;We use convolutional layers for finding the track head because they have locality (i.e. they are aware of the physical layout of the pixels). Combining them with the pooling layers also makes the network essentially translation invariant, which we want because in principle the track head can be anywhere in the image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Once we train our network, we can use it to predict track head locations for samples in our validation dataset. We get predictions that look like what you see in the figure below. You can see the readout of each pixel given by the circles, which scale in both size and color with the amount of charge measured at that location. The predicted track head location is given by the green ellipsoid. You can see that when the prediction is off, the ellipsoid tends to be bigger, indicating that the network is less certain of its prediction. You may notice that the ellipsoids are all aligned with the cardinal axes, even though the actual error in general is not. This is because we have made predictions for the $X$, $Y$, and $Z$ components for the track head independently, so the uncertainty estimates are also independent. To move beyond this, we need to allow for correlated predictions and errors. This should also make the network more accurate, and “only” requires changing the posterior from a normal-inverse-gamma distribution to a multivariate-normal-inverse-Wishart, which is a higher dimensional generalization of the normal-inverse-gamma distribution. This is a project for another day though!&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif.png&quot; alt=&quot;Foo&quot; /&gt;

  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif_xy.png&quot; alt=&quot;Foo&quot; /&gt;

&lt;/figure&gt;
&lt;figure class=&quot;half&quot;&gt;
  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif_xz.png&quot; alt=&quot;Foo&quot; /&gt;

  
&lt;img src=&quot;/assets/images/20_cm_least_err_serif.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;The uncertainty estimate (green ellipsoid) tends to be larger when the error is larger in each dimension. These are 3 views of the same event.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The below figure shows the predicted squared error against the true error for the validation dataset. If the network were estimating its errors perfectly, this data would follow the simplest parabola $\sigma^2_p = \epsilon_t^2$. The fit here is a little too wide, but the general trend is accurate, meaning that the network is successfully estimating its epistemic uncertainty, albeit undershooting it in general.&lt;/p&gt;

&lt;figure class=&quot;three-quarter&quot;&gt;
  
&lt;img src=&quot;/assets/images/predicted_vs_squared_error_1000_keV_talk_ticks.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;This histogram is fitted to a parabola with no linear component.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If you want to see more details of this analysis, check out the paper, published soon in the Astrophysical Journal (I hope!).&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A muon is a fundamental particle sort of like a heavier version of an electron. Unlike an electron, a muon is not stable, and will decay to an electron and two neutrinos after about 2 milliseconds if it’s at rest. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Micah Buuck</name></author><category term="Blog" /><category term="Physics" /><category term="Machine Learning" /><summary type="html">Uncertainty in Physics Experimental physicists spend most of their research time doing 2 things: building their experiment, and trying to figure out how wrong their results could be. You’ll notice that I skipped right over the part where you actually do your experiment and analyze your data. It’s not that those things don’t happen, it’s just that trying to figure out what you don’t know or didn’t account for is usually a lot harder than doing the thing you set out to do! After all, a result or measurement doesn’t mean much if you don’t know how wrong it could be. The Muon G-2 experiment is located at Fermilab National Accelerator Laboratory in Batavia, Illinois. This image shows the muon storage ring that is the primary component of the experiment. You can see that it's pretty big. This image is taken from their website. Take the case of the Muon g-2 experiment at Fermilab. This experiment was built make an extreeeemely precise measurement of something called the “anomalous magentic moment” of the muon1. The reason physicists are interested in measuring this thing? In large part, it’s because the value that is predicted by particle theory does not agree with the number that has been measured with experiments. But it’s not just the fact that the numbers are different that is interesting, it’s the fact that experimentalists have been able to measure this number to incredible precision, and it’s waaay off from what theorists and their integrals say it should be. Uncertainty estimation (in this case, very small uncertainty estimation), is the linchpin of this experiment. Physicists are also very excited to use machine learning in their research. However, most machine learning algorithms do not include any way to estimate the uncertainty of their predictions. This is a huge problem for physicists if they want to use the output of a machine learning model to infer anything interesting. To be sure, there are use cases in physics for machine learning with no uncertainty estimation, but you probably can’t unlock the full potential of machine learning models if you have to constrain the use of them to the parts of an analysis where uncertainty quantification is not necessary. Uncertainty in Machine Learning Self-driving cars are only as safe as their algorithms. This image is taken from this electrek article. A lack of uncertainty quantification in machine learning is also a huge problem for other people, with an obvious example being the people who are trying to create self-driving cars. If your car pulls up to an intersection and thinks there’s no stop sign, should it still stop? How sure is it that there’s no stop sign? Is it worse for a car to accidentally stop at an intersection where there is not actually a stop sign, or to blow through a stop sign at an intersection where there could be pedestrians expecting it to stop? If the decision-making algorithm the car is using can only say “there is a stop sign” or “there is not a stop sign”, then it’s always going to treat those two possible mistakes as equally bad, even though the car speeding though a stop sign is clearly much worse than stopping (or at least slowing down) at an intersection with no stop sign. Ideally you want the car to say to itself “I can’t really tell if there’s a stop sign here, so I’ll slow down and keep looking until I can get a better idea”. You may recognize this as what probably goes through your head in this situation as well. We’re all doing uncertainty quantification all the time! Being such an important topic, it’s unsurprising that Uncertainty Quantification (UQ) is a hot topic these days in machine learning research. There are a bunch of different approaches to quantify predictive uncertainty in your machine learning model, but I’m going to describe one that I have used in the past, and that I think is elegant and seems to work quite well! The rest of the sections in this post require some understanding of Bayesian statistics, so if you are not familiar with those words, or what a prior or posterior distribution are, check out this other post I wrote. Flavors of Uncertainty When physicists quantify their uncertainties, they usually break them up into two components: statistical and systematic. Statistical uncertainty refers to the fact that, because you only get to do one experiment to measure something (or at least a fixed number of them), you aren’t going to get the exact true value of the parameter you are trying to measure from your data. There’s a canonical example where you are trying to estimate the average height of a group of people by measuring their heights one at a time. Your best guess is going to be the average of the heights you have measured (assuming you are randomly sampling from the population of people), but the average height of the few people you’ve actually measured isn’t going to be the exact same as the average height of the whole group. Statistical uncertainty represents that difference. Systematic uncertainty is sort of a catch-all for all other types of possible sources of error, like: is the ruler you are using to measure people accurate? does the set of people you have measured have any systematic differences with the full population (i.e. you aren’t sampling truly randomly)? could the average height of the population be changing over time (e.g. lots of babies born)? Thomas Bayes would like you to quantify your knowledge please. This image is from his Wikipedia page. In Machine Learning, uncertainty is also broken up into two components that are in many ways similar to the way it is done in physics. They are: aleatoric and epistemic. Aleatoric uncertainty is essentially a measure of the variance of your data. If the population whose average height you are trying to measure comes from all ages, that group will likely have a high level of aleatoric uncertainty, while if they are all the same age and sex, the aleatoric uncertainty will probably be lower. Epistemic uncertainty refers to uncertainty in the model itself, i.e. how confident a model is in its own predictions. Estimating the average height of a population does not require a complex model, but if you are trying to do something more complicated, like classifying pictures of animals by species, you will probably need a much more complex model that will have some degree of uncertainty when it makes predictions. In many situations, as described above, having access to this source of uncertainty can be very useful. GammaTPC and Evidential Deep Learning I’ve been working on a project to design a space-based telescope to look for gamma-rays of a particular type that have been a bit neglected for the last few decades. This telescope will contain a bunch of liquid argon, where the gamma-rays will scatter off of electrons in the argon. When they scatter, the hit electron starts flying through the argon, leaving behind a trail of destruction (i.e. other ionized electrons) before it eventually runs out of energy and stops. If we can precisely measure the initial locations of several scatters, we can use physics and math to reconstruct where the gamma-ray must have come from. Because these scattered electrons are leaving a cloud/track of ionized electrons behind, we need to be able to determine where the beginning of the track is to successfully make this measurement. This telescope will be able to produce a 3D-grid reconstruction of the electron track, so it’s plausible that a 3D convolutional neural network will be able to do a good job in reconstructing the location of the initial scatter. Left: A gamma-ray will scatter multiple times in the detector, and if you can figure out the order and locations, you can tell which direction it came from. Right: An example electron track (line), and the pixelated readout that the detector actually sees (circles). The circles are sized proportionally to the amount of charge measured at that location. Precisely measuring the position of these electron scatters is key, but obtaining an uncertainty estimate for the observations is also very useful. If we know that a particular electron scatter is poorly measured, for whatever reason, it can be better to just completely ignore that data point, rather than include it in our data set knowing that it may be throwing off our measurements. I and several collaborators at SLAC have a paper out where we did just that. You can also find Jupyter notebooks to recreate the analysis here. We tested several different kinds of uncertainty estimating models on our problem, and determined that an approach from 2020, called Evidential Deep Learning, worked the best for us. Evidential Deep Learning works well because it makes use of the handy mathematical relationships between conjugate distributions that I discuss in another post to give you an uncertainty estimate without having to run the model a bunch of times (like an ensemble-based approach), or use a Bayesian neural network which can be much more computationally expensive. We also found that it performed better than other options in some key statistical metrics. We are trying to predict the location of an electron scatter from a 3D grid showing the track that it left as it traveled through its argon medium. Minimally, this requires a model that produces 3 real numbers as output: predictions for the x, y, and z components of the location. These can be compared to the true simulated locations, and if the loss function (i.e. likelihood) being used is the mean-squared-error, that comparison should follow a normal distribution. In Evidential Deep Learning, we no longer try to just predict 3 numbers, but instead we actually try to predict 12 numbers, 4 for each cardinal direction prediction. These numbers are the parameters characterizing a normal-inverse-gamma distribution, which is the cojugate prior for a normal likelihood with an unknown mean and unknown variance (i.e. our likelihood). In this case, the normal likelihood in some sense represents the distribution of our data set, with the mean being predictions for the electron track head locations, and the variance being the aleatoric uncertainty. The normal-inverse-gamma posterior then is effectively the epistemic uncertainty the model has for both its predictions and its estimate of the aleatoric uncertainty. When we train our model to predict all of the parameters for the posterior distribution (called hyperparameters), we are essentially training it to estimate a range of plausible predictions for each electron track, rather than just a single point estimate. After the model is trained, we can feed in new examples of electron tracks, and then use the predicted normal-inverse-gamma hyperparameters to get a prediction interval. A normal-inverse-gamma distribution is a joint probability distribution, which is the product of two unidimensional distributions. It has 2 random variables, and so its density function is 2D. To find the heads of the electron tracks, we designed a multi-layer 3D convolutional neural network with 3 convolutional layers, 2 max pooling layers, and 1 global average pooling layer, followed by a feed-forward layer producing the final predictions. The model shown in the figure below is for a deterministic model we created as a benchmark to measure the Evidential model against. You can tell this because it ends with a length 3 vector for predicting the $X$, $Y$, and $Z$ components of the location. A very handy feature of Evidential Deep Learning is that it only requires swapping out the final layer for one that predicts 4x the number of outputs, and then altering the loss function appropriately. Furthermore, the author of the original paper has written a Python package that has code that can essentially be dropped into your existing setup. Instead of using the mean-squared-error for a loss function, you use the negative-log-likelihood (NLL) of the normal-inverse-gamma distribution, since minimizing the NLL produces the same optimized parameters as maximizing the likelihood does. And maximizing the likelihood while training your neural network essentially means that you are maximizing the ability of your network to predict the correct outputs from a given set of inputs. We use convolutional layers for finding the track head because they have locality (i.e. they are aware of the physical layout of the pixels). Combining them with the pooling layers also makes the network essentially translation invariant, which we want because in principle the track head can be anywhere in the image. Once we train our network, we can use it to predict track head locations for samples in our validation dataset. We get predictions that look like what you see in the figure below. You can see the readout of each pixel given by the circles, which scale in both size and color with the amount of charge measured at that location. The predicted track head location is given by the green ellipsoid. You can see that when the prediction is off, the ellipsoid tends to be bigger, indicating that the network is less certain of its prediction. You may notice that the ellipsoids are all aligned with the cardinal axes, even though the actual error in general is not. This is because we have made predictions for the $X$, $Y$, and $Z$ components for the track head independently, so the uncertainty estimates are also independent. To move beyond this, we need to allow for correlated predictions and errors. This should also make the network more accurate, and “only” requires changing the posterior from a normal-inverse-gamma distribution to a multivariate-normal-inverse-Wishart, which is a higher dimensional generalization of the normal-inverse-gamma distribution. This is a project for another day though! The uncertainty estimate (green ellipsoid) tends to be larger when the error is larger in each dimension. These are 3 views of the same event. The below figure shows the predicted squared error against the true error for the validation dataset. If the network were estimating its errors perfectly, this data would follow the simplest parabola $\sigma^2_p = \epsilon_t^2$. The fit here is a little too wide, but the general trend is accurate, meaning that the network is successfully estimating its epistemic uncertainty, albeit undershooting it in general. This histogram is fitted to a parabola with no linear component. If you want to see more details of this analysis, check out the paper, published soon in the Astrophysical Journal (I hope!). A muon is a fundamental particle sort of like a heavier version of an electron. Unlike an electron, a muon is not stable, and will decay to an electron and two neutrinos after about 2 milliseconds if it’s at rest. &amp;#8617;</summary></entry><entry><title type="html">Basic Bayesian Statistics</title><link href="http://localhost:4000/blog/bayes-theorem/" rel="alternate" type="text/html" title="Basic Bayesian Statistics" /><published>2022-10-29T00:00:00-07:00</published><updated>2022-10-29T00:00:00-07:00</updated><id>http://localhost:4000/blog/bayes-theorem</id><content type="html" xml:base="http://localhost:4000/blog/bayes-theorem/">&lt;h3 id=&quot;bayess-theorem&quot;&gt;Bayes’s Theorem&lt;/h3&gt;

&lt;p&gt;I am an ancestral Minnesota Twins fan, having grown up in Minnesota during the glory days of the M&amp;amp;M boys, Johan Santana, and Ron Gardenhire. Back then, the Twins won a lot of games, just…not during the postseason. In fact, the Twins have never had much success in October, making the playoffs only 14 times out of 61 seasons. In that time, they’ve compiled a 25-44 record, which is not good, but given that they’re always playing the best teams in the playoffs, it’s not horrific. Unfortunately, most of those losses have come since the last time they won the World Series in 1991, and shockingly, 16 of them are to a single team: the New York Yankees, who they’ve beaten exactly twice in that time.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/relief-pitcher-mariano-rivera-of-the-new-york-yankees-news-photo-102500888-1548242521.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;This face was bad news for Twins fans. Photo by Christian Petersen // Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, let’s say you’re a Twins fan, and you’re excited that your team has made it to the playoffs, and you don’t yet know what team they’ll be facing, but it could be the Yankees. What is the probability that the Twins will win the first game they play? The most straightforward way to predict this is to assume that current trends will hold: they will probably lose (25/(25+44) = 36% chance of winning), but they are much more likely to lose if they are playing the Yankees (2/(2+16) = 11% chance of winning). Bayes’s theorem is a theorem invented by Thomas Bayes (shock) and refined and published by Richard Price that provides a formalism for this kind of conditional thinking. It starts with the idea of a “conditional probability”, or in our example, what is the probability that the Twins are going to win a playoff game given that we know they are playing the Yankees?&lt;/p&gt;

\[\begin{align*}
P(W|Y) = \frac{P(W\cap Y)}{P(Y)}
\end{align*}\]

&lt;p&gt;That equation essentially reads “The probability ($P$) that the Twins will win ($W$) if they are playing the Yankees ($Y$) is the same as the fraction of playoff games where the Twins have played and beaten the Yankees ($P(W\cap Y)$)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; divided by the fraction of playoff games the Twins have played against the Yankees ($P(Y)$).”&lt;/p&gt;

&lt;p&gt;Let’s say you missed the game and didn’t even know who the Twins were playing, but you hear the next day that the Twins won. You might then ask yourself (if you are a nerd), “Given that I know the Twins won their game last night, what is the probability it was against the Yankees?” In that case the equation would be:&lt;/p&gt;

\[\begin{align*}
P(Y|W) = \frac{P(W\cap Y)}{P(W)}
\end{align*}\]

&lt;p&gt;A.k.a.: “The probability that the Twins were playing the Yankees, given that I know they won last night, are the same as the number of times the Twins have played and beaten the Yankees divided by the number of playoff games the Twins have ever won.”&lt;/p&gt;

&lt;p&gt;If you’re perceptive you might notice that $P(W\cap Y)$ appears in both equations. We can solve for $P(W\cap Y)$ in one equation and stick it into the other, arriving at Bayes Theorem:&lt;/p&gt;

\[\begin{align*}
P(W|Y) = \frac{P(Y|W) P(W)}{P(Y)}
\end{align*}\]

&lt;p&gt;This is a slightly more confusing equation that reads “The probability that the Twins win their playoff game given that it is against the Yankees, is the same as the probability that the Twins played the Yankees given that we know they won that game, multiplied by the fraction of playoff games the Twins have ever won, divided by the fraction of total playoff games the Twins have played against the Yankees.”&lt;/p&gt;

&lt;p&gt;This sentence is confusing, but isn’t really necessary to understand in detail, because the point of this equation is to use it to do statistics. Scientists most frequently use this theorem in the context of understanding how the data they get from their experiments change their model/theory of the phenomenon they are investigating.&lt;/p&gt;

&lt;p&gt;For example, let’s say you are a physicist who is studying a new particle, and you want to measure its mass. Let’s say you already have some idea of what the mass is: you have a most likely value for the mass, and some idea of how good that estimate is in the first place. You can model this understanding as a normal distribution, centered at your best estimate of the mass, and with a standard deviation that quantifies your understanding of how good the estimate is.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/normal-distribution.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Several examples of normal distributions. Figure author: Wikipedia user Inductiveload.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now let’s say you go make 15 measurements of the mass of this particle. Bayes Theorem gives you a way to take in those measurements, and use them to “update” your model of the particle’s mass. Maybe the mean of the distribution will change, and hopefully the standard deviation will decrease (i.e. your understanding improves). To do this, Bayes Theorem tells you to compute the “likelihood” of getting the measurements given your current model of the particle mass, and multiply it by the model itself (called the “prior” distrubution). Because the final thing you are getting (the “posterior” distribution) is still a probability distribution, you then have to normalize it to 1 (this is how I understand $P(Y)$ in this example). This procedure for taking a model of something, incorporating new data, and computing the updated model given the data, is extremely powerful.&lt;/p&gt;

&lt;h3 id=&quot;conjugate-priors&quot;&gt;Conjugate priors&lt;/h3&gt;

&lt;p&gt;In certain situations, multiplying the prior and likelihood distributions together to get the posterior distribution can, by stroke of fortuitous math, return you a distribution with the same form as the prior distribution. This means that the mathematics of propagating likelihood information through a Bayesian model is very straightforward, and even better, can be repeated without the form of the posterior becoming a total disaster. One of these conjugate prior-posterior relationships occurs when you are trying to estimate the mean of a normal distribution.&lt;/p&gt;

&lt;p&gt;We can return to the model from the previous section, where our physicist (you) is trying to measure the mass of a particle. The way physicists typically measure the mass of a particle is to watch it break up into other product particles, and then measure the total energy of all of the products. It turns out that, due to measurement error, and even quantum mechanics, the total energy of these products does not always add up to the exact same number, even if the starting particle is the same every time. This means that if we measure the total energy of the products a bunch of times and plot the distribution, we will get a normal distribution, where the mean represents the mass of the particle, and the standard deviation represents its decay rate, or the inverse of its half-life (i.e. a stable particle like an electron would appear ).&lt;/p&gt;

&lt;p&gt;If we take the Bayesian approach, and say that we do not &lt;em&gt;know&lt;/em&gt; what the mean is, but rather have a prior for it, choosing that prior distribution to also be a normal distribution will give us a conjugate prior-posterior relationship. Let’s work it out:&lt;/p&gt;

&lt;p&gt;The mass of our particle has some value $\mu$, and the width has some value $\Gamma$. The profile of the particle is then&lt;/p&gt;

\[\begin{align*}
P(x | \mu, \Gamma) = \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x-\mu)^2}{2\Gamma^2}
\end{align*}\]

&lt;p&gt;Our conjugate prior on $\mu$ has, in this case, the same form:&lt;/p&gt;

\[\begin{align*}
p_0(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}e^\frac{-(\mu-\mu_0)^2}{2\sigma_0^2}
\end{align*}\]

&lt;p&gt;What we need now is the likelihood, which is really similar to $P\left(x | \mu, \Gamma\right)$ but where you insert the values you have actually measured for $x$, which we’ll now call $\textbf{x} = {x_1, x_2, … x_n}$ for $n$ observations. Since we are computing the likelihood of measuring all of those observations, we will just multiply all of them together:&lt;/p&gt;

\[\begin{align*}
L = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x_i-\mu)^2}{2\Gamma^2} \\
L = \frac{1}{\left(2\pi\Gamma^2\right)^{\frac{n}{2}}} e^\frac{\sum_{i=1}^{n}-(x_i - \mu)^2}{2\Gamma^2} \\
\end{align*}\]

&lt;p&gt;After multiplying this by $p_0(\mu)$ and refactoring, we get:&lt;/p&gt;

\[\begin{align*}
p_1(\mu) = \frac{1}{\sqrt{2\pi\sigma_1^2}}e^\frac{-(\mu-\mu_1)^2}{2\sigma_1^2}
\end{align*}\]

&lt;p&gt;where $\sigma_1 = \left( \frac{1}{\sigma_0^2} + \frac{n}{\Gamma^2} \right)^{-1}$ and $\mu_1 = \sigma_1 \left( \frac{\mu_0}{\sigma_0^2}+\frac{\sum_{i=1}^n x_i}{\Gamma^2} \right)$. In the limiting case where $n \rightarrow \infty$, this reduces to $\sigma_1 \rightarrow \frac{\Gamma^2}{n} \rightarrow 0$, and $\mu_1 \rightarrow \frac{\Gamma^2}{n}\frac{\sum_{i=1}^n x_i}{\Gamma^2} \rightarrow \bar{\textbf{x}}$, meaning that our estimate of $\mu$ gets arbitrarily precise over time. You can do the exact same thing if you assume that $\mu$ is known but $\Gamma$ is not, in which case the conjugate prior is an inverse-gamma distribution. If you want to place a prior on both $\mu$ and $\Gamma$ simulaneously, you can do this with a normal-inverse-gamma distribution.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;$\cap$ is a math symbol that basically means “and”. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Micah Buuck</name></author><category term="Blog" /><category term="Statistics" /><category term="Machine Learning" /><summary type="html">Bayes’s Theorem I am an ancestral Minnesota Twins fan, having grown up in Minnesota during the glory days of the M&amp;amp;M boys, Johan Santana, and Ron Gardenhire. Back then, the Twins won a lot of games, just…not during the postseason. In fact, the Twins have never had much success in October, making the playoffs only 14 times out of 61 seasons. In that time, they’ve compiled a 25-44 record, which is not good, but given that they’re always playing the best teams in the playoffs, it’s not horrific. Unfortunately, most of those losses have come since the last time they won the World Series in 1991, and shockingly, 16 of them are to a single team: the New York Yankees, who they’ve beaten exactly twice in that time. This face was bad news for Twins fans. Photo by Christian Petersen // Getty Images So, let’s say you’re a Twins fan, and you’re excited that your team has made it to the playoffs, and you don’t yet know what team they’ll be facing, but it could be the Yankees. What is the probability that the Twins will win the first game they play? The most straightforward way to predict this is to assume that current trends will hold: they will probably lose (25/(25+44) = 36% chance of winning), but they are much more likely to lose if they are playing the Yankees (2/(2+16) = 11% chance of winning). Bayes’s theorem is a theorem invented by Thomas Bayes (shock) and refined and published by Richard Price that provides a formalism for this kind of conditional thinking. It starts with the idea of a “conditional probability”, or in our example, what is the probability that the Twins are going to win a playoff game given that we know they are playing the Yankees? \[\begin{align*} P(W|Y) = \frac{P(W\cap Y)}{P(Y)} \end{align*}\] That equation essentially reads “The probability ($P$) that the Twins will win ($W$) if they are playing the Yankees ($Y$) is the same as the fraction of playoff games where the Twins have played and beaten the Yankees ($P(W\cap Y)$)1 divided by the fraction of playoff games the Twins have played against the Yankees ($P(Y)$).” Let’s say you missed the game and didn’t even know who the Twins were playing, but you hear the next day that the Twins won. You might then ask yourself (if you are a nerd), “Given that I know the Twins won their game last night, what is the probability it was against the Yankees?” In that case the equation would be: \[\begin{align*} P(Y|W) = \frac{P(W\cap Y)}{P(W)} \end{align*}\] A.k.a.: “The probability that the Twins were playing the Yankees, given that I know they won last night, are the same as the number of times the Twins have played and beaten the Yankees divided by the number of playoff games the Twins have ever won.” If you’re perceptive you might notice that $P(W\cap Y)$ appears in both equations. We can solve for $P(W\cap Y)$ in one equation and stick it into the other, arriving at Bayes Theorem: \[\begin{align*} P(W|Y) = \frac{P(Y|W) P(W)}{P(Y)} \end{align*}\] This is a slightly more confusing equation that reads “The probability that the Twins win their playoff game given that it is against the Yankees, is the same as the probability that the Twins played the Yankees given that we know they won that game, multiplied by the fraction of playoff games the Twins have ever won, divided by the fraction of total playoff games the Twins have played against the Yankees.” This sentence is confusing, but isn’t really necessary to understand in detail, because the point of this equation is to use it to do statistics. Scientists most frequently use this theorem in the context of understanding how the data they get from their experiments change their model/theory of the phenomenon they are investigating. For example, let’s say you are a physicist who is studying a new particle, and you want to measure its mass. Let’s say you already have some idea of what the mass is: you have a most likely value for the mass, and some idea of how good that estimate is in the first place. You can model this understanding as a normal distribution, centered at your best estimate of the mass, and with a standard deviation that quantifies your understanding of how good the estimate is. Several examples of normal distributions. Figure author: Wikipedia user Inductiveload. Now let’s say you go make 15 measurements of the mass of this particle. Bayes Theorem gives you a way to take in those measurements, and use them to “update” your model of the particle’s mass. Maybe the mean of the distribution will change, and hopefully the standard deviation will decrease (i.e. your understanding improves). To do this, Bayes Theorem tells you to compute the “likelihood” of getting the measurements given your current model of the particle mass, and multiply it by the model itself (called the “prior” distrubution). Because the final thing you are getting (the “posterior” distribution) is still a probability distribution, you then have to normalize it to 1 (this is how I understand $P(Y)$ in this example). This procedure for taking a model of something, incorporating new data, and computing the updated model given the data, is extremely powerful. Conjugate priors In certain situations, multiplying the prior and likelihood distributions together to get the posterior distribution can, by stroke of fortuitous math, return you a distribution with the same form as the prior distribution. This means that the mathematics of propagating likelihood information through a Bayesian model is very straightforward, and even better, can be repeated without the form of the posterior becoming a total disaster. One of these conjugate prior-posterior relationships occurs when you are trying to estimate the mean of a normal distribution. We can return to the model from the previous section, where our physicist (you) is trying to measure the mass of a particle. The way physicists typically measure the mass of a particle is to watch it break up into other product particles, and then measure the total energy of all of the products. It turns out that, due to measurement error, and even quantum mechanics, the total energy of these products does not always add up to the exact same number, even if the starting particle is the same every time. This means that if we measure the total energy of the products a bunch of times and plot the distribution, we will get a normal distribution, where the mean represents the mass of the particle, and the standard deviation represents its decay rate, or the inverse of its half-life (i.e. a stable particle like an electron would appear ). If we take the Bayesian approach, and say that we do not know what the mean is, but rather have a prior for it, choosing that prior distribution to also be a normal distribution will give us a conjugate prior-posterior relationship. Let’s work it out: The mass of our particle has some value $\mu$, and the width has some value $\Gamma$. The profile of the particle is then \[\begin{align*} P(x | \mu, \Gamma) = \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x-\mu)^2}{2\Gamma^2} \end{align*}\] Our conjugate prior on $\mu$ has, in this case, the same form: \[\begin{align*} p_0(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}e^\frac{-(\mu-\mu_0)^2}{2\sigma_0^2} \end{align*}\] What we need now is the likelihood, which is really similar to $P\left(x | \mu, \Gamma\right)$ but where you insert the values you have actually measured for $x$, which we’ll now call $\textbf{x} = {x_1, x_2, … x_n}$ for $n$ observations. Since we are computing the likelihood of measuring all of those observations, we will just multiply all of them together: \[\begin{align*} L = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x_i-\mu)^2}{2\Gamma^2} \\ L = \frac{1}{\left(2\pi\Gamma^2\right)^{\frac{n}{2}}} e^\frac{\sum_{i=1}^{n}-(x_i - \mu)^2}{2\Gamma^2} \\ \end{align*}\] After multiplying this by $p_0(\mu)$ and refactoring, we get: \[\begin{align*} p_1(\mu) = \frac{1}{\sqrt{2\pi\sigma_1^2}}e^\frac{-(\mu-\mu_1)^2}{2\sigma_1^2} \end{align*}\] where $\sigma_1 = \left( \frac{1}{\sigma_0^2} + \frac{n}{\Gamma^2} \right)^{-1}$ and $\mu_1 = \sigma_1 \left( \frac{\mu_0}{\sigma_0^2}+\frac{\sum_{i=1}^n x_i}{\Gamma^2} \right)$. In the limiting case where $n \rightarrow \infty$, this reduces to $\sigma_1 \rightarrow \frac{\Gamma^2}{n} \rightarrow 0$, and $\mu_1 \rightarrow \frac{\Gamma^2}{n}\frac{\sum_{i=1}^n x_i}{\Gamma^2} \rightarrow \bar{\textbf{x}}$, meaning that our estimate of $\mu$ gets arbitrarily precise over time. You can do the exact same thing if you assume that $\mu$ is known but $\Gamma$ is not, in which case the conjugate prior is an inverse-gamma distribution. If you want to place a prior on both $\mu$ and $\Gamma$ simulaneously, you can do this with a normal-inverse-gamma distribution. $\cap$ is a math symbol that basically means “and”. &amp;#8617;</summary></entry></feed>