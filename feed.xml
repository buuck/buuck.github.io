<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-26T12:07:35-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Micah Buuck, PhD</title><subtitle>The personal website of Micah Buuck. I'm a Research Associate at SLAC National Accelerator Laboratory, working at the intersection of physics and machine learning. Here you can find posts and write-ups of things I've worked on.</subtitle><author><name>Micah Buuck</name></author><entry><title type="html">World Electricity And Air Pollution</title><link href="http://localhost:4000/world-electricity-and-air-pollution/" rel="alternate" type="text/html" title="World Electricity And Air Pollution" /><published>2023-05-25T00:00:00-07:00</published><updated>2023-05-25T00:00:00-07:00</updated><id>http://localhost:4000/world-electricity-and-air-pollution</id><content type="html" xml:base="http://localhost:4000/world-electricity-and-air-pollution/">&lt;h2 id=&quot;connecting-world-electricity-production-with-air-pollution&quot;&gt;Connecting world electricity production with air pollution&lt;/h2&gt;

&lt;p&gt;It is reasonable to assume that particularly dirty energy production, e.g. coal, will be associated with higher death rates from air pollution. This notebook investigates the relationship between different kinds of electricity production and deaths from air pollution using data from Our World in Data. I find that the relationship between electricity production and air pollution deaths is not quite so straightforward. You can also find this notebook, including the code and data, on &lt;a href=&quot;https://www.github.com/buuck/world_electricity_and_air_pollution&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;load-and-prepare-data&quot;&gt;Load and prepare data&lt;/h3&gt;

&lt;p&gt;I have two shapefile sets, but they both have issues. The One is old and is missing at least one new country (South Sudan). The one in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world_countries_generalized&lt;/code&gt; is better, but for some reason it does not separate China from Taiwan… So I’ll combine them by hand by loading the geometries for China and Taiwan from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world_shapefiles&lt;/code&gt; into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world_countries_generalized&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is a sample from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world_countries_generalized&lt;/code&gt;:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;FID&lt;/th&gt;
      &lt;th&gt;NAME&lt;/th&gt;
      &lt;th&gt;ISO&lt;/th&gt;
      &lt;th&gt;COUNTRYAFF&lt;/th&gt;
      &lt;th&gt;AFF_ISO&lt;/th&gt;
      &lt;th&gt;SHAPE_Leng&lt;/th&gt;
      &lt;th&gt;SHAPE_Area&lt;/th&gt;
      &lt;th&gt;geometry&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;73&lt;/th&gt;
      &lt;td&gt;74&lt;/td&gt;
      &lt;td&gt;Ethiopia&lt;/td&gt;
      &lt;td&gt;ET&lt;/td&gt;
      &lt;td&gt;Ethiopia&lt;/td&gt;
      &lt;td&gt;ET&lt;/td&gt;
      &lt;td&gt;46.810315&lt;/td&gt;
      &lt;td&gt;92.722761&lt;/td&gt;
      &lt;td&gt;POLYGON ((45.48940 5.48976, 45.37446 5.36392, ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;247&lt;/th&gt;
      &lt;td&gt;248&lt;/td&gt;
      &lt;td&gt;Wallis and Futuna&lt;/td&gt;
      &lt;td&gt;WF&lt;/td&gt;
      &lt;td&gt;France&lt;/td&gt;
      &lt;td&gt;FR&lt;/td&gt;
      &lt;td&gt;0.700608&lt;/td&gt;
      &lt;td&gt;0.013414&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-178.06082 -14.32389, -178.137...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;56&lt;/th&gt;
      &lt;td&gt;57&lt;/td&gt;
      &lt;td&gt;Côte d'Ivoire&lt;/td&gt;
      &lt;td&gt;CI&lt;/td&gt;
      &lt;td&gt;Côte d'Ivoire&lt;/td&gt;
      &lt;td&gt;CI&lt;/td&gt;
      &lt;td&gt;31.576752&lt;/td&gt;
      &lt;td&gt;26.340497&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-5.33971 5.19775, -5.31977 5.1...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Armenia&lt;/td&gt;
      &lt;td&gt;AM&lt;/td&gt;
      &lt;td&gt;Armenia&lt;/td&gt;
      &lt;td&gt;AM&lt;/td&gt;
      &lt;td&gt;12.161117&lt;/td&gt;
      &lt;td&gt;3.142291&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((46.54037 38.87559, 46.51639 38...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;246&lt;/th&gt;
      &lt;td&gt;247&lt;/td&gt;
      &lt;td&gt;Vietnam&lt;/td&gt;
      &lt;td&gt;VN&lt;/td&gt;
      &lt;td&gt;Viet Nam&lt;/td&gt;
      &lt;td&gt;VN&lt;/td&gt;
      &lt;td&gt;66.866802&lt;/td&gt;
      &lt;td&gt;27.556082&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((107.07896 17.10804, 107.08333 ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;228&lt;/th&gt;
      &lt;td&gt;229&lt;/td&gt;
      &lt;td&gt;Trinidad and Tobago&lt;/td&gt;
      &lt;td&gt;TT&lt;/td&gt;
      &lt;td&gt;Trinidad and Tobago&lt;/td&gt;
      &lt;td&gt;TT&lt;/td&gt;
      &lt;td&gt;4.384972&lt;/td&gt;
      &lt;td&gt;0.413753&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-61.07945 10.82416, -61.07556 ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;69&lt;/th&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;Equatorial Guinea&lt;/td&gt;
      &lt;td&gt;GQ&lt;/td&gt;
      &lt;td&gt;Equatorial Guinea&lt;/td&gt;
      &lt;td&gt;GQ&lt;/td&gt;
      &lt;td&gt;8.191007&lt;/td&gt;
      &lt;td&gt;2.188207&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((10.41505 1.00250, 10.30861 1.0...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;40&lt;/th&gt;
      &lt;td&gt;41&lt;/td&gt;
      &lt;td&gt;Cameroon&lt;/td&gt;
      &lt;td&gt;CM&lt;/td&gt;
      &lt;td&gt;Cameroon&lt;/td&gt;
      &lt;td&gt;CM&lt;/td&gt;
      &lt;td&gt;41.960596&lt;/td&gt;
      &lt;td&gt;37.972713&lt;/td&gt;
      &lt;td&gt;POLYGON ((10.18107 2.16786, 10.07389 2.16778, ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;162&lt;/th&gt;
      &lt;td&gt;163&lt;/td&gt;
      &lt;td&gt;Niue&lt;/td&gt;
      &lt;td&gt;NU&lt;/td&gt;
      &lt;td&gt;New Zealand&lt;/td&gt;
      &lt;td&gt;NZ&lt;/td&gt;
      &lt;td&gt;0.541413&lt;/td&gt;
      &lt;td&gt;0.021414&lt;/td&gt;
      &lt;td&gt;POLYGON ((-169.89389 -19.14556, -169.93088 -19...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;34&lt;/th&gt;
      &lt;td&gt;35&lt;/td&gt;
      &lt;td&gt;Brunei Darussalam&lt;/td&gt;
      &lt;td&gt;BN&lt;/td&gt;
      &lt;td&gt;Brunei Darussalam&lt;/td&gt;
      &lt;td&gt;BN&lt;/td&gt;
      &lt;td&gt;4.918828&lt;/td&gt;
      &lt;td&gt;0.468299&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((115.01844 4.89579, 114.98915 4...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Here is a sample of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;world_shapefiles&lt;/code&gt;:&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;NAME&lt;/th&gt;
      &lt;th&gt;geometry&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;203&lt;/th&gt;
      &lt;td&gt;United Republic of Tanzania&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((39.68250 -7.99333, 39.65305 -7...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;Burma&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((98.03581 9.78639, 98.03027 9.7...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;59&lt;/th&gt;
      &lt;td&gt;Finland&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((23.70583 59.92722, 23.64944 59...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;166&lt;/th&gt;
      &lt;td&gt;Guinea-Bissau&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-15.88583 11.05222, -15.92556 ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;225&lt;/th&gt;
      &lt;td&gt;Netherlands Antilles&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-68.19528 12.22111, -68.19278 ...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;214&lt;/th&gt;
      &lt;td&gt;Viet Nam&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((106.60027 8.64778, 106.59248 8...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;60&lt;/th&gt;
      &lt;td&gt;Fiji&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-178.70776 -20.67444, -178.715...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;21&lt;/th&gt;
      &lt;td&gt;Bulgaria&lt;/td&gt;
      &lt;td&gt;POLYGON ((27.87917 42.84110, 27.89500 42.80250...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;American Samoa&lt;/td&gt;
      &lt;td&gt;MULTIPOLYGON (((-170.54251 -14.29750, -170.546...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;72&lt;/th&gt;
      &lt;td&gt;Guam&lt;/td&gt;
      &lt;td&gt;POLYGON ((144.70941 13.23500, 144.70245 13.235...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The data table containing the death types is quite large and contains many different kinds of deaths. For now we are only going to look at Outdoor Air Pollution deaths.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;NAME&lt;/th&gt;
      &lt;th&gt;Code&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Outdoor air pollution - OWID - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: High systolic blood pressure - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Diet high in sodium - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Diet low in whole grains - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Alcohol use - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Diet low in fruits - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Unsafe water source - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: High body-mass index - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Unsafe sanitation - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: No access to handwashing facility - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Drug use - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Low bone mineral density - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Vitamin A deficiency - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Child stunting - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Discontinued breastfeeding - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Non-exclusive breastfeeding - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
      &lt;th&gt;Deaths - Cause: All causes - Risk: Iron deficiency - Sex: Both - Age: All Ages (Number)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;1990&lt;/td&gt;
      &lt;td&gt;3169&lt;/td&gt;
      &lt;td&gt;25633&lt;/td&gt;
      &lt;td&gt;1045&lt;/td&gt;
      &lt;td&gt;7077&lt;/td&gt;
      &lt;td&gt;356&lt;/td&gt;
      &lt;td&gt;3185&lt;/td&gt;
      &lt;td&gt;3702&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9518&lt;/td&gt;
      &lt;td&gt;2798&lt;/td&gt;
      &lt;td&gt;4825&lt;/td&gt;
      &lt;td&gt;174&lt;/td&gt;
      &lt;td&gt;389&lt;/td&gt;
      &lt;td&gt;2016&lt;/td&gt;
      &lt;td&gt;7686&lt;/td&gt;
      &lt;td&gt;107&lt;/td&gt;
      &lt;td&gt;2216&lt;/td&gt;
      &lt;td&gt;564&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;1991&lt;/td&gt;
      &lt;td&gt;3222&lt;/td&gt;
      &lt;td&gt;25872&lt;/td&gt;
      &lt;td&gt;1055&lt;/td&gt;
      &lt;td&gt;7149&lt;/td&gt;
      &lt;td&gt;364&lt;/td&gt;
      &lt;td&gt;3248&lt;/td&gt;
      &lt;td&gt;4309&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9489&lt;/td&gt;
      &lt;td&gt;3254&lt;/td&gt;
      &lt;td&gt;5127&lt;/td&gt;
      &lt;td&gt;188&lt;/td&gt;
      &lt;td&gt;389&lt;/td&gt;
      &lt;td&gt;2056&lt;/td&gt;
      &lt;td&gt;7886&lt;/td&gt;
      &lt;td&gt;121&lt;/td&gt;
      &lt;td&gt;2501&lt;/td&gt;
      &lt;td&gt;611&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;1992&lt;/td&gt;
      &lt;td&gt;3395&lt;/td&gt;
      &lt;td&gt;26309&lt;/td&gt;
      &lt;td&gt;1075&lt;/td&gt;
      &lt;td&gt;7297&lt;/td&gt;
      &lt;td&gt;376&lt;/td&gt;
      &lt;td&gt;3351&lt;/td&gt;
      &lt;td&gt;5356&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9528&lt;/td&gt;
      &lt;td&gt;4042&lt;/td&gt;
      &lt;td&gt;5889&lt;/td&gt;
      &lt;td&gt;211&lt;/td&gt;
      &lt;td&gt;393&lt;/td&gt;
      &lt;td&gt;2100&lt;/td&gt;
      &lt;td&gt;8568&lt;/td&gt;
      &lt;td&gt;150&lt;/td&gt;
      &lt;td&gt;3053&lt;/td&gt;
      &lt;td&gt;700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;1993&lt;/td&gt;
      &lt;td&gt;3623&lt;/td&gt;
      &lt;td&gt;26961&lt;/td&gt;
      &lt;td&gt;1103&lt;/td&gt;
      &lt;td&gt;7499&lt;/td&gt;
      &lt;td&gt;389&lt;/td&gt;
      &lt;td&gt;3480&lt;/td&gt;
      &lt;td&gt;7152&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9611&lt;/td&gt;
      &lt;td&gt;5392&lt;/td&gt;
      &lt;td&gt;7007&lt;/td&gt;
      &lt;td&gt;232&lt;/td&gt;
      &lt;td&gt;411&lt;/td&gt;
      &lt;td&gt;2316&lt;/td&gt;
      &lt;td&gt;9875&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;3726&lt;/td&gt;
      &lt;td&gt;773&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;1994&lt;/td&gt;
      &lt;td&gt;3788&lt;/td&gt;
      &lt;td&gt;27658&lt;/td&gt;
      &lt;td&gt;1134&lt;/td&gt;
      &lt;td&gt;7698&lt;/td&gt;
      &lt;td&gt;399&lt;/td&gt;
      &lt;td&gt;3610&lt;/td&gt;
      &lt;td&gt;7192&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9675&lt;/td&gt;
      &lt;td&gt;5418&lt;/td&gt;
      &lt;td&gt;7421&lt;/td&gt;
      &lt;td&gt;247&lt;/td&gt;
      &lt;td&gt;413&lt;/td&gt;
      &lt;td&gt;2665&lt;/td&gt;
      &lt;td&gt;11031&lt;/td&gt;
      &lt;td&gt;204&lt;/td&gt;
      &lt;td&gt;3833&lt;/td&gt;
      &lt;td&gt;812&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 31 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;There are a bunch of missing values in the electricity production data table. For some countries, like Afghanistan, there does not appear to be electricity production data dating all the way back to 1900, which is fine. We will simply ignore those periods of time for countries that do not have them.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;NAME&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Code&lt;/th&gt;
      &lt;th&gt;population&lt;/th&gt;
      &lt;th&gt;gdp&lt;/th&gt;
      &lt;th&gt;biofuel_cons_change_pct&lt;/th&gt;
      &lt;th&gt;biofuel_cons_change_twh&lt;/th&gt;
      &lt;th&gt;biofuel_cons_per_capita&lt;/th&gt;
      &lt;th&gt;biofuel_consumption&lt;/th&gt;
      &lt;th&gt;biofuel_elec_per_capita&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;solar_share_elec&lt;/th&gt;
      &lt;th&gt;solar_share_energy&lt;/th&gt;
      &lt;th&gt;wind_cons_change_pct&lt;/th&gt;
      &lt;th&gt;wind_cons_change_twh&lt;/th&gt;
      &lt;th&gt;wind_consumption&lt;/th&gt;
      &lt;th&gt;wind_elec_per_capita&lt;/th&gt;
      &lt;th&gt;wind_electricity&lt;/th&gt;
      &lt;th&gt;wind_energy_per_capita&lt;/th&gt;
      &lt;th&gt;wind_share_elec&lt;/th&gt;
      &lt;th&gt;wind_share_energy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;1900&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;4832414.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;1901&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;4879685.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;1902&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;4935122.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;1903&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;4998861.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Afghanistan&lt;/td&gt;
      &lt;td&gt;1904&lt;/td&gt;
      &lt;td&gt;AFG&lt;/td&gt;
      &lt;td&gt;5063419.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 129 columns&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;analysis-of-electricity-production-and-air-pollution-deaths&quot;&gt;Analysis of electricity production and air pollution deaths&lt;/h3&gt;
&lt;p&gt;Now we are ready to analyze this data set.&lt;/p&gt;

&lt;p&gt;This plot shows the per-capita electricity use by country in 2019. It mostly tracks how you might expect, with less wealthy countries using less electricity. There are some outliers, like Iceland. This is apparently because there is a very strong aluminum industry in Iceland, which uses a lot of electricity. That, combined with Iceland’s very small population, leads to very high per-capita electricity usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_19_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also see how much coal each country burns to generate electricity on a per capita basis. Australia is a clear outlier with a handful of other countries (China, the United States, South Africa, Kazakhstan, and some Eastern/Southeastern European countries) in a second usage tier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_21_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next let’s look at annual deaths from outdoor air pollution per capita. There is some correlation between this plot and the previous one showing coal usage for electricity per capita (compare China and Eastern Europe), but there are plenty of places where outdoor air pollution is bad even though there is not much coal being used (Africa, the Middle East, etc.).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_23_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s make some more plots to investigate the degree to which deaths from outdoor air pollution are related to fossil fuel use. Because the total number of countries in the world is large, we will focus on analyzing data from the 6 most populous and richest countries. These are nearly mutually exclusive sets which is nice for comparative purposes. There are several very wealthy but very small countries with unusual economies that are not particularly representative (e.g. Qatar, Luxembourg, Singapore), so I am excluding any country with a population less than 10 million. I am also excluding Saudi Arabia becauase, while its population is greater than 10 million, as an oil exporter its electricity source mix is unusually skewed towards oil.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Six most populous countries in the world:

China
India
United States
Indonesia
Pakistan
Brazil

Six richest countries in the world (GDP per capita) with a population
  greater than 10 million people (excl. Saudi Arabia):

United States
Australia
Netherlands
Germany
Sweden
Canada
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below we see a series of plots comparing the annual number of deaths per capita from outdoor air pollution to a variety of metrics, mostly related to electricity production. However, before we start looking at those comparisons, I want to show this plot that motivates what I said above: that the most populous countries are generally not the wealthiest countries. They basically fall into three categories, with China, Indonesia, and Brazil having achieved essentially middle-income status, while India and Pakistan are still relatively poor (but getting richer quickly, particularly India!) and the US is much wealthier than the rest. Among the rich countries, we see that they are all pretty much the same, with the US (and to a lesser extent Australia) being a bit wealthier than the rest.&lt;/p&gt;

&lt;p&gt;It’s also instructive to look at the scales of the x-axes. The wealthy countries are all within ~20% of each other in GDP per capita, while Brazil, Indonesia, and China are all more than twice as wealthy as India and Pakistan.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_32_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we’ll look at the relationships that outdoor air pollution deaths have with different electricity production metrics.&lt;/p&gt;

&lt;p&gt;The first plot compares to the annual amount of electricity generated per capita from coal. One might expect that this would be correlated with deaths from outdoor air pollution, and ideed we see that this is the case for both populous and rich countries. The lightness of color for each country shows the progression over time, with later years being darker. We can see that, apart from the US, the most populous countries in the world have mostly seen increasing deaths from outdoor air pollution over time, which mostly corresponds to an increase in electricity generation from coal. The rich countries are doing the opposite: reducing their coal electricity generation and simultaneously reducing their rates of outdoor air pollution deaths. The Netherlands are somewhat of an outlier among the rich countries, as they seem to be increasing their coal electricity generation over time, but this has not yet lead to an increase in air pollution deaths. It turns out that there are factors besides coal electricity generation that affect air pollution deaths!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_34_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second plot make the same comparison with solar electricity generation per capita. Here we see that both groups of countries are deploying more solar power, but this is of course not correlated with a decrease in deaths from outdoor air pollution in the populous countries. Building out renewables is not enough; we need to reduce emissions!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_36_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we compare to the fraction of electricity that is generated by fossil fuels, we see somewhat different relationships. I’ll note that these plots have a logisticly scaled x-axis to better show the behavior of countries with either very high or very low fractions of fossil fuel electricity generation.&lt;/p&gt;

&lt;p&gt;Even though China is increasing its coal usage for electricity, it is simultaneously reducing the fraction of its electricity that comes from fossil fuels. This is because it is building renewables even faster! We see that most of the other populous countries are mostly holding steady in fossil fuel use, with the exceptions of the US and Brazil. Brazil is unfortunately increasing its fossil fuel use, although it is important to note that this is from an incredibly low baseline.&lt;/p&gt;

&lt;p&gt;The rich countries are all reducing their fossil fuel use overall, but there is wide variation in where they are in that process. Sweden has a nearly completely clean grid, while the Netherlands and Australia have comparitvely dirty grids.
&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_38_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally let’s compare to the fraction of electricity generated from renewable energy sources. Again, I plot the x-axis on a logistic scale. These are basically just inverses of the previous plots. We can see that the Netherlands and Germany have really scaled their renewable energy over the past 30 years or so, while other countries have been slower.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_40_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;extracting-some-numbers-from-the-above-plots&quot;&gt;Extracting some numbers from the above plots&lt;/h3&gt;

&lt;p&gt;Some of the relationships in the above plots look vaguely linear, so let’s see if we can successfully apply that hypothesis to the rest of our data set. First we’ll take a look at the relationship between the share of electricity generated from fossil fuels and the number of deaths from outdoor air pollution per capita. Because the plot we made showing this relationship had log-logistic axes, the relationship in linear space is complicated. Suffice it to say that a positive slope (in the log-logistic space) indicates that deaths from outdoor air pollution generally go up when the fraction of electricty from fossil fuels increases, and a negative slope indicates the reverse. I plot only those countries that had sufficient data to perform a fit that returned a nonzero slope with a p-value less than 0.1. Countries that do not meet these criteria are shown in gray. I’m using a relatively high p-value because this is not a particularly rigorious study and we are more interested in seeing possible relationships than quantifying anything.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_45_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In my opinion, the above figure shows that, in general, weathier countries see a positive relationship between fossil fuel electricity share and deaths from outdoor air pollution. I.e. deaths decrease as fossil fuel use decreases. In particular, I am looking at Europe, North America, and Australia/New Zealand. There are a few outliers in that group, namely Norway, Latvia, Switzerland, Serbia, and North Macedonia. Conversely, many countries in the developing world have a negative relationship between these two variables. I suspect that this is probably related to the fact that as they develop, they are simultaneously increasing fossil fuel usage and improving their public health and healthcare so that illnesses from outdoor air pollution are less likely to be fatal.&lt;/p&gt;

&lt;p&gt;Let’s see to what degree this relationship between this slope and a country’s wealth holds up. Below I plot these slopes against each country’s GDP per capita.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_47_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that there is some relationship between the two, primarily for countries with a GDP per capita above ~$30,000 per year.&lt;/p&gt;

&lt;p&gt;Now let’s look at the relationship between each country’s annual electricity generation from coal and the number of deaths from outdoor air pollution per capita. Again, I plot only those countries that had sufficient data to perform a fit that returned a nonzero slope with a p-value less than 0.1. Countries that do not meet these criteria are shown in gray.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_51_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we see that most countries have a positive relationship between coal electricity generation and deaths from outdoor air pollution. Norway is clearly an outlier, and several countries with apparently negative power-law exponents are also outliers. Below I show a plot comparing some of those countries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/world_electricity_and_air_pollution_files/world_electricity_and_air_pollution_53_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that Norway is an outlier because it has had very little coal in its electricity generation mix for the entirety of this data set. That, combined with its great strides in reducing deaths from air pollution, mean that the slope of this relationship is very large for Norway. Italy and Kazakhstan are also outliers on this plot in that they have negative slopes. For Italy we can see that, even though its fitted slope apparently has a p-value &amp;lt; 0.1, its evolution over time does not seem to be particularly linear. Kazakhstan does seem to be reducing air pollution deaths while increasing its coal usage, perhaps because it is still a developing country.&lt;/p&gt;</content><author><name>Micah Buuck</name></author><summary type="html">Connecting world electricity production with air pollution</summary></entry><entry><title type="html">Caiso Analysis</title><link href="http://localhost:4000/CAISO-Analysis/" rel="alternate" type="text/html" title="Caiso Analysis" /><published>2023-02-05T00:00:00-08:00</published><updated>2023-02-05T00:00:00-08:00</updated><id>http://localhost:4000/CAISO-Analysis</id><content type="html" xml:base="http://localhost:4000/CAISO-Analysis/">&lt;h1 id=&quot;analysis-of-2021-2022-caiso-power-source-data&quot;&gt;Analysis of 2021-2022 CAISO Power Source Data&lt;/h1&gt;

&lt;p&gt;This data comes from &lt;a href=&quot;https://www.kaggle.com/datasets/karatechop/caiso-renewable-energy-data-20212022&quot;&gt;Kaggle&lt;/a&gt;, and presumably ultimately from CAISO. The units are not labeled, so I am guessing a bit here on what is what. I’m going to do some exploratory analysis, and see if we can do any meaningful forecasting.&lt;/p&gt;

&lt;h3 id=&quot;read-and-preprocess-data&quot;&gt;Read and preprocess data&lt;/h3&gt;

&lt;p&gt;Make sure that the date/time columns in the DataFrame are coded as such so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seaborn&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; don’t treat them as strings, which causes major problems when plotting.&lt;/p&gt;

&lt;p&gt;It’s not clear what the units are for the different energy sources, but based on what is on the &lt;a href=&quot;https://www.caiso.com/TodaysOutlook/Pages/default.aspx&quot;&gt;CAISO website&lt;/a&gt;, I’m guessing they’re MW.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Solar&lt;/th&gt;
      &lt;th&gt;Wind&lt;/th&gt;
      &lt;th&gt;Geothermal&lt;/th&gt;
      &lt;th&gt;Biomass&lt;/th&gt;
      &lt;th&gt;Biogas&lt;/th&gt;
      &lt;th&gt;Small hydro&lt;/th&gt;
      &lt;th&gt;Coal&lt;/th&gt;
      &lt;th&gt;Nuclear&lt;/th&gt;
      &lt;th&gt;Natural Gas&lt;/th&gt;
      &lt;th&gt;Large Hydro&lt;/th&gt;
      &lt;th&gt;Batteries&lt;/th&gt;
      &lt;th&gt;Imports&lt;/th&gt;
      &lt;th&gt;DateTime&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Total power&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:00:00&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
      &lt;td&gt;4547.0&lt;/td&gt;
      &lt;td&gt;928.0&lt;/td&gt;
      &lt;td&gt;281.0&lt;/td&gt;
      &lt;td&gt;195.0&lt;/td&gt;
      &lt;td&gt;168.0&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2263.0&lt;/td&gt;
      &lt;td&gt;8875.0&lt;/td&gt;
      &lt;td&gt;1261.0&lt;/td&gt;
      &lt;td&gt;-186.0&lt;/td&gt;
      &lt;td&gt;8145.0&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:00:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;26461.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:05:00&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
      &lt;td&gt;4528.0&lt;/td&gt;
      &lt;td&gt;929.0&lt;/td&gt;
      &lt;td&gt;283.0&lt;/td&gt;
      &lt;td&gt;201.0&lt;/td&gt;
      &lt;td&gt;169.0&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2262.0&lt;/td&gt;
      &lt;td&gt;9086.0&lt;/td&gt;
      &lt;td&gt;1109.0&lt;/td&gt;
      &lt;td&gt;-13.0&lt;/td&gt;
      &lt;td&gt;7717.0&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:05:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;26255.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:10:00&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
      &lt;td&gt;4511.0&lt;/td&gt;
      &lt;td&gt;929.0&lt;/td&gt;
      &lt;td&gt;281.0&lt;/td&gt;
      &lt;td&gt;208.0&lt;/td&gt;
      &lt;td&gt;146.0&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2263.0&lt;/td&gt;
      &lt;td&gt;9168.0&lt;/td&gt;
      &lt;td&gt;985.0&lt;/td&gt;
      &lt;td&gt;37.0&lt;/td&gt;
      &lt;td&gt;7553.0&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:10:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;26065.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:15:00&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
      &lt;td&gt;4514.0&lt;/td&gt;
      &lt;td&gt;929.0&lt;/td&gt;
      &lt;td&gt;280.0&lt;/td&gt;
      &lt;td&gt;214.0&lt;/td&gt;
      &lt;td&gt;140.0&lt;/td&gt;
      &lt;td&gt;19.0&lt;/td&gt;
      &lt;td&gt;2262.0&lt;/td&gt;
      &lt;td&gt;9167.0&lt;/td&gt;
      &lt;td&gt;962.0&lt;/td&gt;
      &lt;td&gt;34.0&lt;/td&gt;
      &lt;td&gt;7458.0&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:15:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;25945.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:20:00&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
      &lt;td&gt;4515.0&lt;/td&gt;
      &lt;td&gt;929.0&lt;/td&gt;
      &lt;td&gt;281.0&lt;/td&gt;
      &lt;td&gt;215.0&lt;/td&gt;
      &lt;td&gt;140.0&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2262.0&lt;/td&gt;
      &lt;td&gt;9176.0&lt;/td&gt;
      &lt;td&gt;949.0&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
      &lt;td&gt;7342.0&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:20:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;25828.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;105099&lt;/th&gt;
      &lt;td&gt;283&lt;/td&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:35:00&lt;/td&gt;
      &lt;td&gt;-1.0&lt;/td&gt;
      &lt;td&gt;2576.0&lt;/td&gt;
      &lt;td&gt;872.0&lt;/td&gt;
      &lt;td&gt;339.0&lt;/td&gt;
      &lt;td&gt;203.0&lt;/td&gt;
      &lt;td&gt;206.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;2263.0&lt;/td&gt;
      &lt;td&gt;16228.0&lt;/td&gt;
      &lt;td&gt;2679.0&lt;/td&gt;
      &lt;td&gt;-434.0&lt;/td&gt;
      &lt;td&gt;8044.0&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:35:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;32979.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;105100&lt;/th&gt;
      &lt;td&gt;284&lt;/td&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:40:00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2589.0&lt;/td&gt;
      &lt;td&gt;871.0&lt;/td&gt;
      &lt;td&gt;340.0&lt;/td&gt;
      &lt;td&gt;204.0&lt;/td&gt;
      &lt;td&gt;206.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2264.0&lt;/td&gt;
      &lt;td&gt;16098.0&lt;/td&gt;
      &lt;td&gt;2687.0&lt;/td&gt;
      &lt;td&gt;-487.0&lt;/td&gt;
      &lt;td&gt;8078.0&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:40:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;32855.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;105101&lt;/th&gt;
      &lt;td&gt;285&lt;/td&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:45:00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2558.0&lt;/td&gt;
      &lt;td&gt;872.0&lt;/td&gt;
      &lt;td&gt;339.0&lt;/td&gt;
      &lt;td&gt;204.0&lt;/td&gt;
      &lt;td&gt;206.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2263.0&lt;/td&gt;
      &lt;td&gt;16034.0&lt;/td&gt;
      &lt;td&gt;2636.0&lt;/td&gt;
      &lt;td&gt;-514.0&lt;/td&gt;
      &lt;td&gt;8120.0&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:45:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;32723.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;105102&lt;/th&gt;
      &lt;td&gt;286&lt;/td&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:50:00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2521.0&lt;/td&gt;
      &lt;td&gt;871.0&lt;/td&gt;
      &lt;td&gt;339.0&lt;/td&gt;
      &lt;td&gt;204.0&lt;/td&gt;
      &lt;td&gt;206.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2264.0&lt;/td&gt;
      &lt;td&gt;15976.0&lt;/td&gt;
      &lt;td&gt;2642.0&lt;/td&gt;
      &lt;td&gt;-521.0&lt;/td&gt;
      &lt;td&gt;8163.0&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:50:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;32670.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;105103&lt;/th&gt;
      &lt;td&gt;287&lt;/td&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:55:00&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2513.0&lt;/td&gt;
      &lt;td&gt;871.0&lt;/td&gt;
      &lt;td&gt;341.0&lt;/td&gt;
      &lt;td&gt;205.0&lt;/td&gt;
      &lt;td&gt;206.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2264.0&lt;/td&gt;
      &lt;td&gt;15581.0&lt;/td&gt;
      &lt;td&gt;2650.0&lt;/td&gt;
      &lt;td&gt;-389.0&lt;/td&gt;
      &lt;td&gt;7993.0&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:55:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;32240.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;105104 rows × 19 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;From the description below, we can see that Natural Gas is clearly the largest power source, and also the second most variable after Solar (2nd highest standard deviation).&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;Solar&lt;/th&gt;
      &lt;th&gt;Wind&lt;/th&gt;
      &lt;th&gt;Geothermal&lt;/th&gt;
      &lt;th&gt;Biomass&lt;/th&gt;
      &lt;th&gt;Biogas&lt;/th&gt;
      &lt;th&gt;Small hydro&lt;/th&gt;
      &lt;th&gt;Coal&lt;/th&gt;
      &lt;th&gt;Nuclear&lt;/th&gt;
      &lt;th&gt;Natural Gas&lt;/th&gt;
      &lt;th&gt;Large Hydro&lt;/th&gt;
      &lt;th&gt;Batteries&lt;/th&gt;
      &lt;th&gt;Imports&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Total power&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;105104.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
      &lt;td&gt;105104.000000&lt;/td&gt;
      &lt;td&gt;105104.000000&lt;/td&gt;
      &lt;td&gt;105092.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;143.478231&lt;/td&gt;
      &lt;td&gt;4193.116555&lt;/td&gt;
      &lt;td&gt;2454.372731&lt;/td&gt;
      &lt;td&gt;869.873949&lt;/td&gt;
      &lt;td&gt;287.631047&lt;/td&gt;
      &lt;td&gt;200.969284&lt;/td&gt;
      &lt;td&gt;191.277966&lt;/td&gt;
      &lt;td&gt;12.498525&lt;/td&gt;
      &lt;td&gt;2076.552973&lt;/td&gt;
      &lt;td&gt;8614.644778&lt;/td&gt;
      &lt;td&gt;1457.924219&lt;/td&gt;
      &lt;td&gt;77.379268&lt;/td&gt;
      &lt;td&gt;5612.521467&lt;/td&gt;
      &lt;td&gt;6.526374&lt;/td&gt;
      &lt;td&gt;2021.665703&lt;/td&gt;
      &lt;td&gt;26048.762760&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;83.125935&lt;/td&gt;
      &lt;td&gt;5046.487510&lt;/td&gt;
      &lt;td&gt;1456.446929&lt;/td&gt;
      &lt;td&gt;76.966457&lt;/td&gt;
      &lt;td&gt;45.488711&lt;/td&gt;
      &lt;td&gt;14.678127&lt;/td&gt;
      &lt;td&gt;94.729647&lt;/td&gt;
      &lt;td&gt;4.994506&lt;/td&gt;
      &lt;td&gt;406.780691&lt;/td&gt;
      &lt;td&gt;3905.161266&lt;/td&gt;
      &lt;td&gt;855.182969&lt;/td&gt;
      &lt;td&gt;572.085363&lt;/td&gt;
      &lt;td&gt;2963.652848&lt;/td&gt;
      &lt;td&gt;3.447995&lt;/td&gt;
      &lt;td&gt;0.471747&lt;/td&gt;
      &lt;td&gt;4762.813769&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-180.000000&lt;/td&gt;
      &lt;td&gt;28.000000&lt;/td&gt;
      &lt;td&gt;474.000000&lt;/td&gt;
      &lt;td&gt;-278.000000&lt;/td&gt;
      &lt;td&gt;132.000000&lt;/td&gt;
      &lt;td&gt;46.000000&lt;/td&gt;
      &lt;td&gt;-6.000000&lt;/td&gt;
      &lt;td&gt;446.000000&lt;/td&gt;
      &lt;td&gt;1494.000000&lt;/td&gt;
      &lt;td&gt;-494.000000&lt;/td&gt;
      &lt;td&gt;-1848.000000&lt;/td&gt;
      &lt;td&gt;-4459.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;2021.000000&lt;/td&gt;
      &lt;td&gt;15916.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;71.000000&lt;/td&gt;
      &lt;td&gt;-33.000000&lt;/td&gt;
      &lt;td&gt;1200.000000&lt;/td&gt;
      &lt;td&gt;823.000000&lt;/td&gt;
      &lt;td&gt;255.000000&lt;/td&gt;
      &lt;td&gt;195.000000&lt;/td&gt;
      &lt;td&gt;153.000000&lt;/td&gt;
      &lt;td&gt;9.000000&lt;/td&gt;
      &lt;td&gt;2250.000000&lt;/td&gt;
      &lt;td&gt;5692.000000&lt;/td&gt;
      &lt;td&gt;878.000000&lt;/td&gt;
      &lt;td&gt;-234.000000&lt;/td&gt;
      &lt;td&gt;3395.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;2021.000000&lt;/td&gt;
      &lt;td&gt;22661.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;143.000000&lt;/td&gt;
      &lt;td&gt;170.000000&lt;/td&gt;
      &lt;td&gt;2211.000000&lt;/td&gt;
      &lt;td&gt;878.000000&lt;/td&gt;
      &lt;td&gt;288.000000&lt;/td&gt;
      &lt;td&gt;204.000000&lt;/td&gt;
      &lt;td&gt;190.000000&lt;/td&gt;
      &lt;td&gt;14.000000&lt;/td&gt;
      &lt;td&gt;2264.000000&lt;/td&gt;
      &lt;td&gt;8116.000000&lt;/td&gt;
      &lt;td&gt;1279.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;6208.000000&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;2022.000000&lt;/td&gt;
      &lt;td&gt;25121.500000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;215.000000&lt;/td&gt;
      &lt;td&gt;9391.000000&lt;/td&gt;
      &lt;td&gt;3649.000000&lt;/td&gt;
      &lt;td&gt;903.000000&lt;/td&gt;
      &lt;td&gt;320.000000&lt;/td&gt;
      &lt;td&gt;211.000000&lt;/td&gt;
      &lt;td&gt;226.000000&lt;/td&gt;
      &lt;td&gt;17.000000&lt;/td&gt;
      &lt;td&gt;2268.000000&lt;/td&gt;
      &lt;td&gt;10828.000000&lt;/td&gt;
      &lt;td&gt;1908.000000&lt;/td&gt;
      &lt;td&gt;319.000000&lt;/td&gt;
      &lt;td&gt;7999.000000&lt;/td&gt;
      &lt;td&gt;10.000000&lt;/td&gt;
      &lt;td&gt;2022.000000&lt;/td&gt;
      &lt;td&gt;27983.250000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;287.000000&lt;/td&gt;
      &lt;td&gt;14288.000000&lt;/td&gt;
      &lt;td&gt;6429.000000&lt;/td&gt;
      &lt;td&gt;1134.000000&lt;/td&gt;
      &lt;td&gt;412.000000&lt;/td&gt;
      &lt;td&gt;242.000000&lt;/td&gt;
      &lt;td&gt;3316.000000&lt;/td&gt;
      &lt;td&gt;91.000000&lt;/td&gt;
      &lt;td&gt;2287.000000&lt;/td&gt;
      &lt;td&gt;25441.000000&lt;/td&gt;
      &lt;td&gt;4556.000000&lt;/td&gt;
      &lt;td&gt;3053.000000&lt;/td&gt;
      &lt;td&gt;11587.000000&lt;/td&gt;
      &lt;td&gt;12.000000&lt;/td&gt;
      &lt;td&gt;2022.000000&lt;/td&gt;
      &lt;td&gt;46679.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;p&gt;This dataset comes in 5 minute intervals spanning a full year. That is &amp;gt;100k samples, which is difficult to visualize meaningfully all at once. Furthermore, there are at least 2 meaningful periods of variation in this dataset: daily and annually. For these reasons, we’ll make 2 different plots, one showing the daily average power generation over the course of the full year, and one showing hourly power generation over one day, averaging over every day of the dataset.&lt;/p&gt;

&lt;p&gt;If we do this for only the Solar power data, we see the following figures&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_11_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_12_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The curves are almost exactly what we should expect. The annual curve peaks in late June at the summer solstice, and has a trough in late December during the winter solstice. The light blue shaded region shows the 95% inner quantile range.&lt;/p&gt;

&lt;p&gt;In order to make more plots easily with Seaborn, we need to convert the DataFrame from wide to long format.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Date&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;DateTime&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Source&lt;/th&gt;
      &lt;th&gt;Power (MW)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:00:00&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:00:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;Solar&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:05:00&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:05:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;Solar&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:10:00&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:10:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;Solar&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:15:00&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:15:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;Solar&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2021-09-01&lt;/td&gt;
      &lt;td&gt;2023-02-05 00:20:00&lt;/td&gt;
      &lt;td&gt;2021-09-01 00:20:00&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2021&lt;/td&gt;
      &lt;td&gt;Solar&lt;/td&gt;
      &lt;td&gt;-34.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1366347&lt;/th&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:35:00&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:35:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;Total power&lt;/td&gt;
      &lt;td&gt;32979.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1366348&lt;/th&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:40:00&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:40:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;Total power&lt;/td&gt;
      &lt;td&gt;32855.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1366349&lt;/th&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:45:00&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:45:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;Total power&lt;/td&gt;
      &lt;td&gt;32723.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1366350&lt;/th&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:50:00&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:50:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;Total power&lt;/td&gt;
      &lt;td&gt;32670.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1366351&lt;/th&gt;
      &lt;td&gt;2022-08-31&lt;/td&gt;
      &lt;td&gt;2023-02-05 23:55:00&lt;/td&gt;
      &lt;td&gt;2022-08-31 23:55:00&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
      &lt;td&gt;Total power&lt;/td&gt;
      &lt;td&gt;32240.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;1366352 rows × 7 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Next we’ll make the same plots as above, but this time for all power sources.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_18_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_19_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a number of interesting features happening in the two figures above.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A weeklong variable period is apparent in the Total Power curve. January 1st 2022 was a Saturday and also lines up with one of the troughs, which indicates to me that weekends generally put substantially less load on the power system.&lt;/li&gt;
  &lt;li&gt;In the linear plot, we can see that the Total Power drawn is generally higher in the summer and lower in the winter, although there is a notable increase in power draw around Christmastime. Christmas lights? Or perhaps it was just cold.&lt;/li&gt;
  &lt;li&gt;Unsurprisingly, nuclear power output is generally extremely steady, although we can see that it did drop off a few times.&lt;/li&gt;
  &lt;li&gt;We can again see the seasonal variability of solar power, probably more clearly in the linear plot than the log.&lt;/li&gt;
  &lt;li&gt;On a day-to-day basis, wind power is extremely variable, more so than solar.&lt;/li&gt;
  &lt;li&gt;California imports more power in the winter than summer. I would guess this may be because electricity gets more expensive in the summer, and therefore harder to import.&lt;/li&gt;
  &lt;li&gt;Biomass, biogas, small hydro, and batteries play a pretty small role.&lt;/li&gt;
  &lt;li&gt;There is virtually no coal power in the system. That’s because there is only one 63 MW coal plant operating in the state of California, in Trona.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let’s take a look at the hourly data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_21_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_22_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again we can make a few interesting observations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Although they make up only a small part of CAISO’s power capacity, batteries are playing a nontrivial role at certain times of day, particularly in the evening. We can also see the batteries charging during the day in the linear plot (where the curve goes negative).&lt;/li&gt;
  &lt;li&gt;The solar power curve has two “tails”. I don’t know what is causing them, but my hypothesis of what is has two components:
    &lt;ul&gt;
      &lt;li&gt;California is a long state N/S, and summer is the part of the year where the sun is up latest into the evening. During the summer, the evening terminator is mostly perpendicular to the length of the state, meaning that there should be a gradual dropoff in solar power as it gets dark from south to north. That explains the fact that there is at least one bump.&lt;/li&gt;
      &lt;li&gt;There are two bumps instead of one because of daylight savings time.&lt;/li&gt;
      &lt;li&gt;This does not occur in the morning because on summer mornings, the terminator faces the opposite direction and so all parts of the state start receiving solar power at about the same time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Wind power does indeed see variation complementary to solar power as advertised, but it is a much smaller source than solar and therefore is not able to offset much of the solar variability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the last plots in this section, I’m going to separate sources into dispatchable and variable. Dispatchable resources are those that can be adjusted during the day to compensate for uncontrollable variability in other sources and demand. In practice, I am separating sources into these two categories based on whether they appear to have controllable daily variation in the plot above. The dispatchable resources that seem to have this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Natural Gas&lt;/li&gt;
  &lt;li&gt;Imports&lt;/li&gt;
  &lt;li&gt;Large Hydro&lt;/li&gt;
  &lt;li&gt;Small Hydro&lt;/li&gt;
  &lt;li&gt;Batteries&lt;/li&gt;
  &lt;li&gt;(Theoretically coal would go here as well, but it’s too small for me to see evidence of daily variation.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In principle, some of the other resources can be ramped up or down, but I would guess that they aren’t because they are smaller and more distributed, and may not be technically set up to be ramped on demand.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_26_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_27_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s not quite as much to see here as in the previous plots, except that the duck curve is alive and well in the hourly plot. The daily plot is dominated by random variation and the seasonal variation of solar power.&lt;/p&gt;

&lt;h2 id=&quot;time-series-modeling&quot;&gt;Time-series Modeling&lt;/h2&gt;

&lt;p&gt;Let’s see if we can extract any utility (haha) from time-series analysis of this data. First we’ll try an ARIMA analysis.&lt;/p&gt;

&lt;p&gt;We have a full year of 5-minute samples available, which translates to 105,104 samples. This (empirially) is too many to fit on my laptop, so I’m going to downsample the data to hourly. In the real world, 5-minute predictions may be useful, but we’re going to make do with 1-hr predictions for now.&lt;/p&gt;

&lt;p&gt;We’re going to try to predict the total amount of power from dispatchable resources required at any point in time. This is similar to what a real power company or ISO has to do in real time: tune the dispatchable power sources to cover the difference between variable supply and demand at all times.&lt;/p&gt;

&lt;p&gt;Since the amount of variable resource supply affects how much dispatchable power will be needed, it is reasonable to look for relationships between previous values of variable power output and the current amount of dispatchable power. Below I create a series of plots showing the relationship between a series of lagged variable power samples and dispatchable power.&lt;/p&gt;

&lt;p&gt;First let’s take a look at the autocorrelation and partial autocorrelation functions. These will help us decide what kind of model is most appropriate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_36_2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_36_3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see pretty clear evidence of a daily pattern in the autocorrelation function. The partial autocorrelation function suggests that there is meaningful information to be extracted up to lags of about 24, which makes sense since this data is taken over a 24 hour daily period. Unsurprisingly, we see evidence of “seasonality”, where in this case a season corresponds to a 24 hr day. This suggests that an ARIMA model of ARIMA(24, 0, 0)x(1, 0, 0, 24) could be appropriate. However, this is way too many lags to fit in a reasonable amount of time, so we’ll just have to see how many we can do before I run out of computation power or they stop adding reasonable predictive power.&lt;/p&gt;

&lt;p&gt;We should also look at the real-time relationship between variable and dispatchable resources. Clearly there is a strong relationship, but if we are trying to predict future values of dispatchable power, we will only have access to past values of variable power. Therefore, we need to take at least one lag to make this realistic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_39_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_40_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_41_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_42_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_43_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first two variable lags have a reasonably strong relationship with the dispatchable power output, but by the time we get to the third lag, there isn’t much left. The ninth lag also shows essentially no relationship between the two, so we’ll limit the model to the first two lags of variable power output.&lt;/p&gt;

&lt;p&gt;Given the results of the autocorrelation plots and the Variable vs. Dispatchable plots above, let’s build a series of models with 1 and 2 Variable source lags, building up the number of autocorrelated lags as far as we reasonably can. We’ll also include one 24-hr seasonal term.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                               SARIMAX Results                                
==============================================================================
Dep. Variable:           Dispatchable   No. Observations:                 8759
Model:                 ARIMA(1, 0, 0)   Log Likelihood              -79321.828
Date:                Sun, 05 Feb 2023   AIC                         158649.656
Time:                        10:16:15   BIC                         158670.890
Sample:                             0   HQIC                        158656.891
                               - 8759                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const       1.597e+04    420.877     37.936      0.000    1.51e+04    1.68e+04
ar.L1          0.9439      0.004    239.155      0.000       0.936       0.952
sigma2        4.3e+06   5.13e+04     83.832      0.000     4.2e+06     4.4e+06
===================================================================================
Ljung-Box (L1) (Q):                4591.85   Jarque-Bera (JB):              1675.00
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               1.03   Skew:                             0.66
Prob(H) (two-sided):                  0.50   Kurtosis:                         4.69
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).




                               SARIMAX Results                                
==============================================================================
Dep. Variable:           Dispatchable   No. Observations:                 8759
Model:             ARIMA(1, 0, 0, 24)   Log Likelihood              -80386.596
Date:                Sun, 05 Feb 2023   AIC                         160779.192
Time:                        10:16:20   BIC                         160800.425
Sample:                             0   HQIC                        160786.427
                               - 8759                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const       1.597e+04    358.556     44.530      0.000    1.53e+04    1.67e+04
ar.S.L24       0.9298      0.004    236.051      0.000       0.922       0.938
sigma2      5.462e+06   6.02e+04     90.769      0.000    5.34e+06    5.58e+06
===================================================================================
Ljung-Box (L1) (Q):                8069.84   Jarque-Bera (JB):              1334.72
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               1.12   Skew:                             0.08
Prob(H) (two-sided):                  0.00   Kurtosis:                         4.91
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).



                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(1, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -69337.252
Date:                          Sun, 05 Feb 2023   AIC                         138682.504
Time:                                  10:05:54   BIC                         138710.815
Sample:                                       0   HQIC                        138692.150
                                         - 8759                                         
Covariance Type:                            opg                                         
==============================================================================
                 coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const       1.597e+04   1.48e-11   1.08e+15      0.000     1.6e+04     1.6e+04
ar.L1          1.0000    1.6e-05   6.23e+04      0.000       1.000       1.000
ar.S.L24       0.9491      0.002    415.030      0.000       0.945       0.954
sigma2      4.363e+05   1.99e-10    2.2e+15      0.000    4.36e+05    4.36e+05
===================================================================================
Ljung-Box (L1) (Q):                1712.24   Jarque-Bera (JB):             12230.48
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.70   Skew:                             0.04
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.79
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
[2] Covariance matrix is singular or near-singular, with condition number 3.27e+30. Standard errors may be unstable.



                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(1, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -69250.928
Date:                          Sun, 05 Feb 2023   AIC                         138511.856
Time:                                  10:17:19   BIC                         138547.245
Sample:                                       0   HQIC                        138523.915
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const        2.301e+04   8.65e-09   2.66e+12      0.000     2.3e+04     2.3e+04
Variable.L1    -0.4229      0.000   -898.260      0.000      -0.424      -0.422
ar.L1           1.0000   1.77e-05   5.64e+04      0.000       1.000       1.000
ar.S.L24        0.9987      0.001   1724.759      0.000       0.998       1.000
sigma2       4.925e+05   7.78e-10   6.33e+14      0.000    4.92e+05    4.92e+05
===================================================================================
Ljung-Box (L1) (Q):                 401.93   Jarque-Bera (JB):             11334.48
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.75   Skew:                            -0.11
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.57
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
[2] Covariance matrix is singular or near-singular, with condition number 1.1e+28. Standard errors may be unstable.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ar.L1 == 1&lt;/code&gt; is a random walk, where the most likely next value is the current value. On top of that, we have a strong relationship between the next value and the sample 24 hours ago, which is also close to a random walk with a 24 hr period. Finally, we also see that there is an inverse relationship between the forecasted dispatchable power output and the previous variable power output, as expected. We’ll use the information criteria in the upper right to choose our model.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(2, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68125.930
Date:                          Sun, 05 Feb 2023   AIC                         136263.861
Time:                                  07:26:56   BIC                         136306.328
Sample:                                       0   HQIC                        136278.331
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const        2.301e+04   1527.241     15.066      0.000       2e+04     2.6e+04
Variable.L1     0.0347      0.013      2.749      0.006       0.010       0.059
ar.L1           1.4371      0.010    150.823      0.000       1.418       1.456
ar.L2          -0.4938      0.010    -50.632      0.000      -0.513      -0.475
ar.S.L24        0.9263      0.003    328.655      0.000       0.921       0.932
sigma2       3.317e+05   2769.802    119.763      0.000    3.26e+05    3.37e+05
===================================================================================
Ljung-Box (L1) (Q):                  23.19   Jarque-Bera (JB):             11503.60
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.75   Skew:                            -0.10
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.61
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The parameters have changed a bit now that we’ve added the extra lag. Besides the fit taking quite a bit longer, we now have a negative coefficient for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L2&lt;/code&gt; term and a positive coefficient for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L1&lt;/code&gt; term. This corresponds roughly to making a linear extrapolation from the previous two sample points. The coefficient of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Variable.L1&lt;/code&gt; term has also nearly disappeared, but it is still quite statistically significant.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(3, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68048.912
Date:                          Sun, 05 Feb 2023   AIC                         136111.824
Time:                                  10:20:22   BIC                         136161.369
Sample:                                       0   HQIC                        136128.706
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const        2.301e+04   1802.777     12.763      0.000    1.95e+04    2.65e+04
Variable.L1     0.1533      0.016      9.590      0.000       0.122       0.185
ar.L1           1.5700      0.014    108.283      0.000       1.542       1.598
ar.L2          -0.7789      0.025    -30.808      0.000      -0.828      -0.729
ar.L3           0.1616      0.013     12.646      0.000       0.137       0.187
ar.S.L24        0.9258      0.003    328.230      0.000       0.920       0.931
sigma2        3.26e+05   2735.865    119.140      0.000    3.21e+05    3.31e+05
===================================================================================
Ljung-Box (L1) (Q):                   0.20   Jarque-Bera (JB):             11390.09
Prob(Q):                              0.65   Prob(JB):                         0.00
Heteroskedasticity (H):               0.77   Skew:                            -0.08
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.58
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).




                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(4, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68061.618
Date:                          Sun, 05 Feb 2023   AIC                         136139.236
Time:                                  07:51:19   BIC                         136195.859
Sample:                                       0   HQIC                        136158.529
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const        2.301e+04   2668.673      8.622      0.000    1.78e+04    2.82e+04
Variable.L1     0.2007      0.017     12.149      0.000       0.168       0.233
ar.L1           1.6240      0.015    106.777      0.000       1.594       1.654
ar.L2          -0.8668      0.029    -29.703      0.000      -0.924      -0.810
ar.L3           0.2462      0.023     10.583      0.000       0.201       0.292
ar.L4          -0.0364      0.010     -3.502      0.000      -0.057      -0.016
ar.S.L24        0.9268      0.003    329.352      0.000       0.921       0.932
sigma2       3.321e+05   2853.443    116.401      0.000    3.27e+05    3.38e+05
===================================================================================
Ljung-Box (L1) (Q):                   1.26   Jarque-Bera (JB):             12334.06
Prob(Q):                              0.26   Prob(JB):                         0.00
Heteroskedasticity (H):               0.77   Skew:                            -0.06
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.81
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Non-convergence means that we are done adding terms, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_1_3&lt;/code&gt; is the best we are going to do with only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Variable.L1&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(1, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68898.499
Date:                          Sun, 05 Feb 2023   AIC                         137808.998
Time:                                  10:28:14   BIC                         137851.465
Sample:                                       0   HQIC                        137823.468
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const         2.21e+04   2.71e-08   8.16e+11      0.000    2.21e+04    2.21e+04
Variable.L1    -0.3860      0.009    -42.275      0.000      -0.404      -0.368
Variable.L2     0.1625      0.008     20.866      0.000       0.147       0.178
ar.L1           1.0000   5.91e-07   1.69e+06      0.000       1.000       1.000
ar.S.L24        0.9338      0.003    353.493      0.000       0.929       0.939
sigma2       3.995e+05    8.4e-09   4.76e+13      0.000       4e+05       4e+05
===================================================================================
Ljung-Box (L1) (Q):                 507.22   Jarque-Bera (JB):              9530.79
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.76   Skew:                             0.10
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.11
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
[2] Covariance matrix is singular or near-singular, with condition number 2.78e+28. Standard errors may be unstable.




                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(2, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68275.361
Date:                          Sun, 05 Feb 2023   AIC                         136564.722
Time:                                  08:09:44   BIC                         136614.267
Sample:                                       0   HQIC                        136581.604
                                         - 8759                                         
Covariance Type:                            opg                                         
===============================================================================
                  coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------
const         2.21e+04   3.35e-08   6.59e+11      0.000    2.21e+04    2.21e+04
Variable.L1     0.0411      0.010      4.307      0.000       0.022       0.060
Variable.L2     0.1649      0.009     18.478      0.000       0.147       0.182
ar.L1           1.4987      0.009    169.438      0.000       1.481       1.516
ar.L2          -0.4987      0.009    -56.384      0.000      -0.516      -0.481
ar.S.L24        0.9265      0.003    353.314      0.000       0.921       0.932
sigma2       3.426e+05   7.36e-09   4.66e+13      0.000    3.43e+05    3.43e+05
===================================================================================
Ljung-Box (L1) (Q):                  12.19   Jarque-Bera (JB):             12506.26
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               0.76   Skew:                            -0.02
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.85
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
[2] Covariance matrix is singular or near-singular, with condition number 1.51e+29. Standard errors may be unstable.




                                    SARIMAX Results                                     
========================================================================================
Dep. Variable:                     Dispatchable   No. Observations:                 8759
Model:             ARIMA(4, 0, 0)x(1, 0, 0, 24)   Log Likelihood              -68073.396
Date:                          Sun, 05 Feb 2023   AIC                         136160.792
Time:                                  08:14:48   BIC                         136210.337
Sample:                                       0   HQIC                        136177.673
                                         - 8759                                         
Covariance Type:                            opg                                         
==============================================================================
                 coef    std err          z      P&amp;gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const       1.597e+04   1329.281     12.011      0.000    1.34e+04    1.86e+04
ar.L1          1.4725      0.008    185.271      0.000       1.457       1.488
ar.L2         -0.6368      0.015    -42.424      0.000      -0.666      -0.607
ar.L3          0.1400      0.016      8.704      0.000       0.108       0.171
ar.L4         -0.0319      0.010     -3.300      0.001      -0.051      -0.013
ar.S.L24       0.9174      0.003    316.633      0.000       0.912       0.923
sigma2      3.279e+05   2754.964    119.008      0.000    3.22e+05    3.33e+05
===================================================================================
Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):             10359.17
Prob(Q):                              0.96   Prob(JB):                         0.00
Heteroskedasticity (H):               0.78   Skew:                            -0.06
Prob(H) (two-sided):                  0.00   Kurtosis:                         8.33
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can plot a barchart of the AIC values for each model, which will help us decide which is optimal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_60_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Relative AIC is just the difference between the given model’s AIC and the lowest AIC amongst all models. It is a bit hard to see, but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(3)xSAR(1)xVar.L1&lt;/code&gt; model has the lowest AIC, followed closely by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(4)xSAR(1)&lt;/code&gt; model. The fact that these are so close, coupled with the fact that including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Var.L1&lt;/code&gt; does seem to improve things (see &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(1)xSAR(1)&lt;/code&gt; vs. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(1)xSAR(1)xVar.L1&lt;/code&gt;), suggests that a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(4)xSAR(1)xVar.L1&lt;/code&gt; model would likely perform even better if it had converged. Interestingly, adding the second &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Variable&lt;/code&gt; resource lag (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Var.L2&lt;/code&gt;) seems to make things worse at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(2)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next we’ll look directly at some predictions and residuals. First, we’ll make a plot showing the predicted power vs the actual dispatchable power over the course of the year.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_63_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_64_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The residuals are small enough that they don’t noticeably show up on the first plot. By eye it looks like the residuals may be higher in the summer than in the winter, but that is hard to say. That wouldn’t surprise me very much, since solar power is much stronger in the summer and makes up the largest share of the variable resources. Weather effects, which should be a large contributor to solar variablity, would have a bigger effect on the overall market during periods with greater sunshine.&lt;/p&gt;

&lt;p&gt;Next we’ll look at some plots of 3 arbitrary days of data with different models, comparing the actual and predicted dispatchable power output. First we’ll look at the final chosen model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_66_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see excellent agreement in general, as expected.&lt;/p&gt;

&lt;p&gt;Next we’ll look at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(1)&lt;/code&gt; model, a.k.a. one autoregressive lag, no seasonal component, and no dependence on prior variable resource output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_68_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This model tends to predict slight mean reversion based on the previous sample, which makes sense on average, but is pretty bad in general.&lt;/p&gt;

&lt;p&gt;The next model includes only the “seasonal” (a.k.a. daily) term.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_70_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The prediction of this model is only informed by the value at the same time the day before, so while it doesn’t show the lagged behavior that the previous plot does, it is not able to react to any information more recent than 24 hours ago, and consequently is not very good.&lt;/p&gt;

&lt;p&gt;Finally, let’s plot the residuals of these models as well as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AR(1)xSAR(1)xVar.L1&lt;/code&gt; model together. We can see that both the green and red curve perform significanty better than the blue and orange curves, showing that having at least 4 terms (not including the constant term) in the model is important. Red and green are quite similar, but we know from the box-plot above that the more complex model (red) has a lower AIC than green. Since it also has more parameters, that must mean its likelihood is higher, and therefore it should have smaller residuals on average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CAISO_analysis_files/CAISO_analysis_72_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It would be interesting to fit an RNN to this data.&lt;/p&gt;</content><author><name>Micah Buuck</name></author><summary type="html">Analysis of 2021-2022 CAISO Power Source Data</summary></entry><entry><title type="html">Climate Impacts of Scientific Computing in Particle Physics</title><link href="http://localhost:4000/blog/computing-and-climate/" rel="alternate" type="text/html" title="Climate Impacts of Scientific Computing in Particle Physics" /><published>2022-12-06T00:00:00-08:00</published><updated>2022-12-06T00:00:00-08:00</updated><id>http://localhost:4000/blog/computing-and-climate</id><content type="html" xml:base="http://localhost:4000/blog/computing-and-climate/">&lt;h3 id=&quot;snowmass-2021&quot;&gt;Snowmass 2021&lt;/h3&gt;
&lt;p&gt;From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022.&lt;/p&gt;

&lt;p&gt;As a part of this process, I worked on a &lt;a href=&quot;https://arxiv.org/abs/2203.12389&quot; target=&quot;_blank&quot;&gt;white paper&lt;/a&gt; that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share.&lt;/p&gt;

&lt;h2 id=&quot;climate-impacts-of-scientific-computing-in-particle-physics&quot;&gt;Climate Impacts of Scientific Computing in Particle Physics&lt;/h2&gt;

&lt;p&gt;High-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by &lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009324&quot; target=&quot;_blank&quot;&gt;Lannelongue et al.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective.&lt;/p&gt;

&lt;p&gt;Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website &lt;a href=&quot;https://www.green-algorithms.org&quot; target=&quot;_blank&quot;&gt;green-algorithms.org&lt;/a&gt; can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available.&lt;/p&gt;

&lt;p&gt;Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the &lt;a href=&quot;https://www.top500.org/lists/green500/&quot; target=&quot;_blank&quot;&gt;Green500&lt;/a&gt;, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), &lt;a href=&quot;https://www.eia.gov/beta/states/states/fl/overview&quot; target=&quot;_blank&quot;&gt;80.3% of electricity consumption in Florida in 2019 was fossil fueled&lt;/a&gt; while &lt;a href=&quot;https://www.eia.gov/beta/states/states/ca/overview&quot; target=&quot;_blank&quot;&gt;only 35.9% California’s electricity consumption over the same period was fossil fueled&lt;/a&gt;, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions.&lt;/p&gt;

&lt;p&gt;Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8622871&quot; target=&quot;_blank&quot;&gt;Ahmed et al.&lt;/a&gt; have completed a study of this possibility.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Micah Buuck</name></author><category term="Blog" /><category term="Physics" /><category term="Climate Change" /><category term="Computing" /><summary type="html">Snowmass 2021 From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022. As a part of this process, I worked on a white paper that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share. Climate Impacts of Scientific Computing in Particle Physics High-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by Lannelongue et al. The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective. Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website green-algorithms.org can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available. Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the Green500, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), 80.3% of electricity consumption in Florida in 2019 was fossil fueled while only 35.9% California’s electricity consumption over the same period was fossil fueled, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.1 While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions. Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. Ahmed et al. have completed a study of this possibility. This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. &amp;#8617;</summary></entry><entry><title type="html">Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept</title><link href="http://localhost:4000/blog/predictive-uncertainty-gammatpc/" rel="alternate" type="text/html" title="Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept" /><published>2022-11-05T00:00:00-07:00</published><updated>2022-11-05T00:00:00-07:00</updated><id>http://localhost:4000/blog/predictive-uncertainty-gammatpc</id><content type="html" xml:base="http://localhost:4000/blog/predictive-uncertainty-gammatpc/">&lt;h3 id=&quot;uncertainty-in-physics&quot;&gt;Uncertainty in Physics&lt;/h3&gt;

&lt;p&gt;Experimental physicists spend most of their research time doing 2 things: building their experiment, and trying to figure out how wrong their results could be. You’ll notice that I skipped right over the part where you actually do your experiment and analyze your data. It’s not that those things don’t happen, it’s just that trying to figure out what you don’t know or didn’t account for is usually a lot harder than doing the thing you set out to do! After all, a result or measurement doesn’t mean much if you don’t know how wrong it could be.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/muon-g-2-17-0188-20.hr_.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;The Muon G-2 experiment is located at Fermilab National Accelerator Laboratory in Batavia, Illinois. This image shows the muon storage ring that is the primary component of the experiment. You can see that it's pretty big. This image is taken from &lt;a href=&quot;https://news.fnal.gov/2019/03/muon-g-2-begins-second-run/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;their website&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Take the case of the &lt;a href=&quot;https://muon-g-2.fnal.gov/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Muon g-2&lt;/a&gt; experiment at Fermilab. This experiment was built make an extreeeemely precise measurement of something called the “anomalous magentic moment” of the muon&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. The reason physicists are interested in measuring this thing? In large part, it’s because the value that is predicted by particle theory does not agree with the number that has been measured with experiments. But it’s not just the fact that the numbers are different that is interesting, it’s the fact that experimentalists have been able to measure this number to incredible precision, and it’s waaay off from what theorists and their integrals say it should be. Uncertainty estimation (in this case, very small uncertainty estimation), is the linchpin of this experiment.&lt;/p&gt;

&lt;p&gt;Physicists are also very excited to use machine learning in their research. However, most machine learning algorithms do not include any way to estimate the uncertainty of their predictions. This is a huge problem for physicists if they want to use the output of a machine learning model to infer anything interesting. To be sure, there are use cases in physics for machine learning with no uncertainty estimation, but you probably can’t unlock the full potential of machine learning models if you have to constrain the use of them to the parts of an analysis where uncertainty quantification is not necessary.&lt;/p&gt;

&lt;h3 id=&quot;uncertainty-in-machine-learning&quot;&gt;Uncertainty in Machine Learning&lt;/h3&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/Tesla-Model-3-Autopilot-crash-highway-1422096078.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Self-driving cars are only as safe as their algorithms. This image is taken from this &lt;a href=&quot;https://electrek.co/2020/06/01/tesla-model-3-crashing-truck-autopilot-video-viral/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;electrek article&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A lack of uncertainty quantification in machine learning is also a huge problem for other people, with an obvious example being the people who are trying to create self-driving cars. If your car pulls up to an intersection and thinks there’s no stop sign, should it still stop? How sure is it that there’s no stop sign? Is it worse for a car to accidentally stop at an intersection where there is not actually a stop sign, or to blow through a stop sign at an intersection where there could be pedestrians expecting it to stop? If the decision-making algorithm the car is using can only say “there is a stop sign” or “there is not a stop sign”, then it’s always going to treat those two possible mistakes as equally bad, even though the car speeding though a stop sign is clearly much worse than stopping (or at least slowing down) at an intersection with no stop sign. Ideally you want the car to say to itself “I can’t really tell if there’s a stop sign here, so I’ll slow down and keep looking until I can get a better idea”. You may recognize this as what probably goes through your head in this situation as well. We’re all doing uncertainty quantification all the time!&lt;/p&gt;

&lt;p&gt;Being such an important topic, it’s unsurprising that Uncertainty Quantification (UQ) is a hot topic these days in machine learning research. There are a bunch of different approaches to quantify predictive uncertainty in your machine learning model, but I’m going to describe one that I have used in the past, and that I think is elegant and seems to work quite well! The rest of the sections in this post require some understanding of Bayesian statistics, so if you are not familiar with those words, or what a prior or posterior distribution are, check out this other post I wrote.&lt;/p&gt;

&lt;h3 id=&quot;flavors-of-uncertainty&quot;&gt;Flavors of Uncertainty&lt;/h3&gt;

&lt;p&gt;When physicists quantify their uncertainties, they usually break them up into two components: statistical and systematic. Statistical uncertainty refers to the fact that, because you only get to do one experiment to measure something (or at least a fixed number of them), you aren’t going to get the exact true value of the parameter you are trying to measure from your data. There’s a canonical example where you are trying to estimate the average height of a group of people by measuring their heights one at a time. Your best guess is going to be the average of the heights you have measured (assuming you are randomly sampling from the population of people), but the average height of the few people you’ve actually measured isn’t going to be the exact same as the average height of the whole group. Statistical uncertainty represents that difference. Systematic uncertainty is sort of a catch-all for all other types of possible sources of error, like: is the ruler you are using to measure people accurate? does the set of people you have measured have any systematic differences with the full population (i.e. you aren’t sampling truly randomly)? could the average height of the population be changing over time (e.g. lots of babies born)?&lt;/p&gt;

&lt;figure class=&quot;smaller&quot;&gt;
  
&lt;img src=&quot;/assets/images/Thomas_Bayes.gif&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Thomas Bayes would like you to quantify your knowledge please. This image is from his &lt;a href=&quot;https://en.wikipedia.org/wiki/Thomas_Bayes&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Wikipedia page&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In Machine Learning, uncertainty is also broken up into two components that are in many ways similar to the way it is done in physics. They are: aleatoric and epistemic. Aleatoric uncertainty is essentially a measure of the variance of your data. If the population whose average height you are trying to measure comes from all ages, that group will likely have a high level of aleatoric uncertainty, while if they are all the same age and sex, the aleatoric uncertainty will probably be lower. Epistemic uncertainty refers to uncertainty in the model itself, i.e. how confident a model is in its own predictions. Estimating the average height of a population does not require a complex model, but if you are trying to do something more complicated, like classifying pictures of animals by species, you will probably need a much more complex model that will have some degree of uncertainty when it makes predictions. In many situations, as described above, having access to this source of uncertainty can be very useful.&lt;/p&gt;

&lt;h3 id=&quot;gammatpc-and-evidential-deep-learning&quot;&gt;GammaTPC and Evidential Deep Learning&lt;/h3&gt;

&lt;p&gt;I’ve been working on a project to design a space-based telescope to look for gamma-rays of a particular type that have been a bit neglected for the last few decades. This telescope will contain a bunch of liquid argon, where the gamma-rays will scatter off of electrons in the argon. When they scatter, the hit electron starts flying through the argon, leaving behind a trail of destruction (i.e. other ionized electrons) before it eventually runs out of energy and stops. If we can precisely measure the initial locations of several scatters, we can use physics and math to reconstruct where the gamma-ray must have come from. Because these scattered electrons are leaving a cloud/track of ionized electrons behind, we need to be able to determine where the beginning of the track is to successfully make this measurement. This telescope will be able to produce a 3D-grid reconstruction of the electron track, so it’s plausible that a 3D convolutional neural network will be able to do a good job in reconstructing the location of the initial scatter.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/event_v2.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Left: A gamma-ray will scatter multiple times in the detector, and if you can figure out the order and locations, you can tell which direction it came from. Right: An example electron track (line), and the pixelated readout that the detector actually sees (circles). The circles are sized proportionally to the amount of charge measured at that location.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Precisely measuring the position of these electron scatters is key, but obtaining an uncertainty estimate for the observations is also very useful. If we know that a particular electron scatter is poorly measured, for whatever reason, it can be better to just completely ignore that data point, rather than include it in our data set knowing that it may be throwing off our measurements. I and several collaborators at SLAC have a &lt;a href=&quot;https://arxiv.org/abs/2207.07805&quot;&gt;paper&lt;/a&gt; out where we did just that. You can also find Jupyter notebooks to recreate the analysis &lt;a href=&quot;https://gitlab.com/probabilistic-uncertainty-for-gammatpc&quot;&gt;here&lt;/a&gt;. We tested several different kinds of uncertainty estimating models on our problem, and determined that an approach from 2020, called Evidential Deep Learning, worked the best for us. Evidential Deep Learning works well because it makes use of the handy mathematical relationships between conjugate distributions that I discuss in &lt;a href=&quot;/blog/bayes-theorem/&quot;&gt;another post&lt;/a&gt; to give you an uncertainty estimate without having to run the model a bunch of times (like an ensemble-based approach), or use a Bayesian neural network which can be much more computationally expensive. We also found that it performed better than other options in some key statistical metrics.&lt;/p&gt;

&lt;p&gt;We are trying to predict the location of an electron scatter from a 3D grid showing the track that it left as it traveled through its argon medium. Minimally, this requires a model that produces 3 real numbers as output: predictions for the x, y, and z components of the location. These can be compared to the true simulated locations, and if the loss function (i.e. likelihood) being used is the mean-squared-error, that comparison should follow a normal distribution. In Evidential Deep Learning, we no longer try to just predict 3 numbers, but instead we actually try to predict 12 numbers, 4 for each cardinal direction prediction. These numbers are the parameters characterizing a normal-inverse-gamma distribution, which is the cojugate prior for a normal likelihood with an unknown mean and unknown variance (i.e. our likelihood). In this case, the normal likelihood in some sense represents the distribution of our data set, with the mean being predictions for the electron track head locations, and the variance being the aleatoric uncertainty. The normal-inverse-gamma posterior then is effectively the epistemic uncertainty the model has for both its predictions and its estimate of the aleatoric uncertainty. When we train our model to predict all of the parameters for the posterior distribution (called hyperparameters), we are essentially training it to estimate a range of plausible predictions for each electron track, rather than just a single point estimate. After the model is trained, we can feed in new examples of electron tracks, and then use the predicted normal-inverse-gamma hyperparameters to get a prediction interval.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/NIG_distribution.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;A normal-inverse-gamma distribution is a joint probability distribution, which is the product of two unidimensional distributions. It has 2 random variables, and so its density function is 2D.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To find the heads of the electron tracks, we designed a multi-layer 3D convolutional neural network with 3 convolutional layers, 2 max pooling layers, and 1 global average pooling layer, followed by a feed-forward layer producing the final predictions. The model shown in the figure below is for a deterministic model we created as a benchmark to measure the Evidential model against. You can tell this because it ends with a length 3 vector for predicting the $X$, $Y$, and $Z$ components of the location. A very handy feature of Evidential Deep Learning is that it only requires swapping out the final layer for one that predicts 4x the number of outputs, and then altering the loss function appropriately. Furthermore, the author of the original paper has written a &lt;a href=&quot;https://github.com/aamini/evidential-deep-learning&quot;&gt;Python package&lt;/a&gt; that has code that can essentially be dropped into your existing setup. Instead of using the mean-squared-error for a loss function, you use the negative-log-likelihood (NLL) of the normal-inverse-gamma distribution, since minimizing the NLL produces the same optimized parameters as maximizing the likelihood does. And maximizing the likelihood while training your neural network essentially means that you are maximizing the ability of your network to predict the correct outputs from a given set of inputs.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/3DCNNDiagram_Alt.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;We use convolutional layers for finding the track head because they have locality (i.e. they are aware of the physical layout of the pixels). Combining them with the pooling layers also makes the network essentially translation invariant, which we want because in principle the track head can be anywhere in the image.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Once we train our network, we can use it to predict track head locations for samples in our validation dataset. We get predictions that look like what you see in the figure below. You can see the readout of each pixel given by the circles, which scale in both size and color with the amount of charge measured at that location. The predicted track head location is given by the green ellipsoid. You can see that when the prediction is off, the ellipsoid tends to be bigger, indicating that the network is less certain of its prediction. You may notice that the ellipsoids are all aligned with the cardinal axes, even though the actual error in general is not. This is because we have made predictions for the $X$, $Y$, and $Z$ components for the track head independently, so the uncertainty estimates are also independent. To move beyond this, we need to allow for correlated predictions and errors. This should also make the network more accurate, and “only” requires changing the posterior from a normal-inverse-gamma distribution to a multivariate-normal-inverse-Wishart, which is a higher dimensional generalization of the normal-inverse-gamma distribution. This is a project for another day though!&lt;/p&gt;

&lt;figure class=&quot;half&quot;&gt;
  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif.png&quot; alt=&quot;Foo&quot; /&gt;

  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif_xy.png&quot; alt=&quot;Foo&quot; /&gt;

&lt;/figure&gt;
&lt;figure class=&quot;half&quot;&gt;
  
&lt;img src=&quot;/assets/images/5_cm_most_err_serif_xz.png&quot; alt=&quot;Foo&quot; /&gt;

  
&lt;img src=&quot;/assets/images/20_cm_least_err_serif.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;The uncertainty estimate (green ellipsoid) tends to be larger when the error is larger in each dimension. These are 3 views of the same event.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The below figure shows the predicted squared error against the true error for the validation dataset. If the network were estimating its errors perfectly, this data would follow the simplest parabola $\sigma^2_p = \epsilon_t^2$. The fit here is a little too wide, but the general trend is accurate, meaning that the network is successfully estimating its epistemic uncertainty, albeit undershooting it in general.&lt;/p&gt;

&lt;figure class=&quot;three-quarter&quot;&gt;
  
&lt;img src=&quot;/assets/images/predicted_vs_squared_error_1000_keV_talk_ticks.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;This histogram is fitted to a parabola with no linear component.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If you want to see more details of this analysis, check out the paper, published soon in the Astrophysical Journal (I hope!).&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A muon is a fundamental particle sort of like a heavier version of an electron. Unlike an electron, a muon is not stable, and will decay to an electron and two neutrinos after about 2 milliseconds if it’s at rest. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Micah Buuck</name></author><category term="Blog" /><category term="Physics" /><category term="Machine Learning" /><summary type="html">Uncertainty in Physics Experimental physicists spend most of their research time doing 2 things: building their experiment, and trying to figure out how wrong their results could be. You’ll notice that I skipped right over the part where you actually do your experiment and analyze your data. It’s not that those things don’t happen, it’s just that trying to figure out what you don’t know or didn’t account for is usually a lot harder than doing the thing you set out to do! After all, a result or measurement doesn’t mean much if you don’t know how wrong it could be. The Muon G-2 experiment is located at Fermilab National Accelerator Laboratory in Batavia, Illinois. This image shows the muon storage ring that is the primary component of the experiment. You can see that it's pretty big. This image is taken from their website. Take the case of the Muon g-2 experiment at Fermilab. This experiment was built make an extreeeemely precise measurement of something called the “anomalous magentic moment” of the muon1. The reason physicists are interested in measuring this thing? In large part, it’s because the value that is predicted by particle theory does not agree with the number that has been measured with experiments. But it’s not just the fact that the numbers are different that is interesting, it’s the fact that experimentalists have been able to measure this number to incredible precision, and it’s waaay off from what theorists and their integrals say it should be. Uncertainty estimation (in this case, very small uncertainty estimation), is the linchpin of this experiment. Physicists are also very excited to use machine learning in their research. However, most machine learning algorithms do not include any way to estimate the uncertainty of their predictions. This is a huge problem for physicists if they want to use the output of a machine learning model to infer anything interesting. To be sure, there are use cases in physics for machine learning with no uncertainty estimation, but you probably can’t unlock the full potential of machine learning models if you have to constrain the use of them to the parts of an analysis where uncertainty quantification is not necessary. Uncertainty in Machine Learning Self-driving cars are only as safe as their algorithms. This image is taken from this electrek article. A lack of uncertainty quantification in machine learning is also a huge problem for other people, with an obvious example being the people who are trying to create self-driving cars. If your car pulls up to an intersection and thinks there’s no stop sign, should it still stop? How sure is it that there’s no stop sign? Is it worse for a car to accidentally stop at an intersection where there is not actually a stop sign, or to blow through a stop sign at an intersection where there could be pedestrians expecting it to stop? If the decision-making algorithm the car is using can only say “there is a stop sign” or “there is not a stop sign”, then it’s always going to treat those two possible mistakes as equally bad, even though the car speeding though a stop sign is clearly much worse than stopping (or at least slowing down) at an intersection with no stop sign. Ideally you want the car to say to itself “I can’t really tell if there’s a stop sign here, so I’ll slow down and keep looking until I can get a better idea”. You may recognize this as what probably goes through your head in this situation as well. We’re all doing uncertainty quantification all the time! Being such an important topic, it’s unsurprising that Uncertainty Quantification (UQ) is a hot topic these days in machine learning research. There are a bunch of different approaches to quantify predictive uncertainty in your machine learning model, but I’m going to describe one that I have used in the past, and that I think is elegant and seems to work quite well! The rest of the sections in this post require some understanding of Bayesian statistics, so if you are not familiar with those words, or what a prior or posterior distribution are, check out this other post I wrote. Flavors of Uncertainty When physicists quantify their uncertainties, they usually break them up into two components: statistical and systematic. Statistical uncertainty refers to the fact that, because you only get to do one experiment to measure something (or at least a fixed number of them), you aren’t going to get the exact true value of the parameter you are trying to measure from your data. There’s a canonical example where you are trying to estimate the average height of a group of people by measuring their heights one at a time. Your best guess is going to be the average of the heights you have measured (assuming you are randomly sampling from the population of people), but the average height of the few people you’ve actually measured isn’t going to be the exact same as the average height of the whole group. Statistical uncertainty represents that difference. Systematic uncertainty is sort of a catch-all for all other types of possible sources of error, like: is the ruler you are using to measure people accurate? does the set of people you have measured have any systematic differences with the full population (i.e. you aren’t sampling truly randomly)? could the average height of the population be changing over time (e.g. lots of babies born)? Thomas Bayes would like you to quantify your knowledge please. This image is from his Wikipedia page. In Machine Learning, uncertainty is also broken up into two components that are in many ways similar to the way it is done in physics. They are: aleatoric and epistemic. Aleatoric uncertainty is essentially a measure of the variance of your data. If the population whose average height you are trying to measure comes from all ages, that group will likely have a high level of aleatoric uncertainty, while if they are all the same age and sex, the aleatoric uncertainty will probably be lower. Epistemic uncertainty refers to uncertainty in the model itself, i.e. how confident a model is in its own predictions. Estimating the average height of a population does not require a complex model, but if you are trying to do something more complicated, like classifying pictures of animals by species, you will probably need a much more complex model that will have some degree of uncertainty when it makes predictions. In many situations, as described above, having access to this source of uncertainty can be very useful. GammaTPC and Evidential Deep Learning I’ve been working on a project to design a space-based telescope to look for gamma-rays of a particular type that have been a bit neglected for the last few decades. This telescope will contain a bunch of liquid argon, where the gamma-rays will scatter off of electrons in the argon. When they scatter, the hit electron starts flying through the argon, leaving behind a trail of destruction (i.e. other ionized electrons) before it eventually runs out of energy and stops. If we can precisely measure the initial locations of several scatters, we can use physics and math to reconstruct where the gamma-ray must have come from. Because these scattered electrons are leaving a cloud/track of ionized electrons behind, we need to be able to determine where the beginning of the track is to successfully make this measurement. This telescope will be able to produce a 3D-grid reconstruction of the electron track, so it’s plausible that a 3D convolutional neural network will be able to do a good job in reconstructing the location of the initial scatter. Left: A gamma-ray will scatter multiple times in the detector, and if you can figure out the order and locations, you can tell which direction it came from. Right: An example electron track (line), and the pixelated readout that the detector actually sees (circles). The circles are sized proportionally to the amount of charge measured at that location. Precisely measuring the position of these electron scatters is key, but obtaining an uncertainty estimate for the observations is also very useful. If we know that a particular electron scatter is poorly measured, for whatever reason, it can be better to just completely ignore that data point, rather than include it in our data set knowing that it may be throwing off our measurements. I and several collaborators at SLAC have a paper out where we did just that. You can also find Jupyter notebooks to recreate the analysis here. We tested several different kinds of uncertainty estimating models on our problem, and determined that an approach from 2020, called Evidential Deep Learning, worked the best for us. Evidential Deep Learning works well because it makes use of the handy mathematical relationships between conjugate distributions that I discuss in another post to give you an uncertainty estimate without having to run the model a bunch of times (like an ensemble-based approach), or use a Bayesian neural network which can be much more computationally expensive. We also found that it performed better than other options in some key statistical metrics. We are trying to predict the location of an electron scatter from a 3D grid showing the track that it left as it traveled through its argon medium. Minimally, this requires a model that produces 3 real numbers as output: predictions for the x, y, and z components of the location. These can be compared to the true simulated locations, and if the loss function (i.e. likelihood) being used is the mean-squared-error, that comparison should follow a normal distribution. In Evidential Deep Learning, we no longer try to just predict 3 numbers, but instead we actually try to predict 12 numbers, 4 for each cardinal direction prediction. These numbers are the parameters characterizing a normal-inverse-gamma distribution, which is the cojugate prior for a normal likelihood with an unknown mean and unknown variance (i.e. our likelihood). In this case, the normal likelihood in some sense represents the distribution of our data set, with the mean being predictions for the electron track head locations, and the variance being the aleatoric uncertainty. The normal-inverse-gamma posterior then is effectively the epistemic uncertainty the model has for both its predictions and its estimate of the aleatoric uncertainty. When we train our model to predict all of the parameters for the posterior distribution (called hyperparameters), we are essentially training it to estimate a range of plausible predictions for each electron track, rather than just a single point estimate. After the model is trained, we can feed in new examples of electron tracks, and then use the predicted normal-inverse-gamma hyperparameters to get a prediction interval. A normal-inverse-gamma distribution is a joint probability distribution, which is the product of two unidimensional distributions. It has 2 random variables, and so its density function is 2D. To find the heads of the electron tracks, we designed a multi-layer 3D convolutional neural network with 3 convolutional layers, 2 max pooling layers, and 1 global average pooling layer, followed by a feed-forward layer producing the final predictions. The model shown in the figure below is for a deterministic model we created as a benchmark to measure the Evidential model against. You can tell this because it ends with a length 3 vector for predicting the $X$, $Y$, and $Z$ components of the location. A very handy feature of Evidential Deep Learning is that it only requires swapping out the final layer for one that predicts 4x the number of outputs, and then altering the loss function appropriately. Furthermore, the author of the original paper has written a Python package that has code that can essentially be dropped into your existing setup. Instead of using the mean-squared-error for a loss function, you use the negative-log-likelihood (NLL) of the normal-inverse-gamma distribution, since minimizing the NLL produces the same optimized parameters as maximizing the likelihood does. And maximizing the likelihood while training your neural network essentially means that you are maximizing the ability of your network to predict the correct outputs from a given set of inputs. We use convolutional layers for finding the track head because they have locality (i.e. they are aware of the physical layout of the pixels). Combining them with the pooling layers also makes the network essentially translation invariant, which we want because in principle the track head can be anywhere in the image. Once we train our network, we can use it to predict track head locations for samples in our validation dataset. We get predictions that look like what you see in the figure below. You can see the readout of each pixel given by the circles, which scale in both size and color with the amount of charge measured at that location. The predicted track head location is given by the green ellipsoid. You can see that when the prediction is off, the ellipsoid tends to be bigger, indicating that the network is less certain of its prediction. You may notice that the ellipsoids are all aligned with the cardinal axes, even though the actual error in general is not. This is because we have made predictions for the $X$, $Y$, and $Z$ components for the track head independently, so the uncertainty estimates are also independent. To move beyond this, we need to allow for correlated predictions and errors. This should also make the network more accurate, and “only” requires changing the posterior from a normal-inverse-gamma distribution to a multivariate-normal-inverse-Wishart, which is a higher dimensional generalization of the normal-inverse-gamma distribution. This is a project for another day though! The uncertainty estimate (green ellipsoid) tends to be larger when the error is larger in each dimension. These are 3 views of the same event. The below figure shows the predicted squared error against the true error for the validation dataset. If the network were estimating its errors perfectly, this data would follow the simplest parabola $\sigma^2_p = \epsilon_t^2$. The fit here is a little too wide, but the general trend is accurate, meaning that the network is successfully estimating its epistemic uncertainty, albeit undershooting it in general. This histogram is fitted to a parabola with no linear component. If you want to see more details of this analysis, check out the paper, published soon in the Astrophysical Journal (I hope!). A muon is a fundamental particle sort of like a heavier version of an electron. Unlike an electron, a muon is not stable, and will decay to an electron and two neutrinos after about 2 milliseconds if it’s at rest. &amp;#8617;</summary></entry><entry><title type="html">Basic Bayesian Statistics</title><link href="http://localhost:4000/blog/bayes-theorem/" rel="alternate" type="text/html" title="Basic Bayesian Statistics" /><published>2022-10-29T00:00:00-07:00</published><updated>2022-10-29T00:00:00-07:00</updated><id>http://localhost:4000/blog/bayes-theorem</id><content type="html" xml:base="http://localhost:4000/blog/bayes-theorem/">&lt;h3 id=&quot;bayess-theorem&quot;&gt;Bayes’s Theorem&lt;/h3&gt;

&lt;p&gt;I am an ancestral Minnesota Twins fan, having grown up in Minnesota during the glory days of the M&amp;amp;M boys, Johan Santana, and Ron Gardenhire. Back then, the Twins won a lot of games, just…not during the postseason. In fact, the Twins have never had much success in October, making the playoffs only 14 times out of 61 seasons. In that time, they’ve compiled a 25-44 record, which is not good, but given that they’re always playing the best teams in the playoffs, it’s not horrific. Unfortunately, most of those losses have come since the last time they won the World Series in 1991, and shockingly, 16 of them are to a single team: the New York Yankees, who they’ve beaten exactly twice in that time.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/relief-pitcher-mariano-rivera-of-the-new-york-yankees-news-photo-102500888-1548242521.jpg&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;This face was bad news for Twins fans. Photo by Christian Petersen // Getty Images&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, let’s say you’re a Twins fan, and you’re excited that your team has made it to the playoffs, and you don’t yet know what team they’ll be facing, but it could be the Yankees. What is the probability that the Twins will win the first game they play? The most straightforward way to predict this is to assume that current trends will hold: they will probably lose (25/(25+44) = 36% chance of winning), but they are much more likely to lose if they are playing the Yankees (2/(2+16) = 11% chance of winning). Bayes’s theorem is a theorem invented by Thomas Bayes (shock) and refined and published by Richard Price that provides a formalism for this kind of conditional thinking. It starts with the idea of a “conditional probability”, or in our example, what is the probability that the Twins are going to win a playoff game given that we know they are playing the Yankees?&lt;/p&gt;

\[\begin{align*}
P(W|Y) = \frac{P(W\cap Y)}{P(Y)}
\end{align*}\]

&lt;p&gt;That equation essentially reads “The probability ($P$) that the Twins will win ($W$) if they are playing the Yankees ($Y$) is the same as the fraction of playoff games where the Twins have played and beaten the Yankees ($P(W\cap Y)$)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; divided by the fraction of playoff games the Twins have played against the Yankees ($P(Y)$).”&lt;/p&gt;

&lt;p&gt;Let’s say you missed the game and didn’t even know who the Twins were playing, but you hear the next day that the Twins won. You might then ask yourself (if you are a nerd), “Given that I know the Twins won their game last night, what is the probability it was against the Yankees?” In that case the equation would be:&lt;/p&gt;

\[\begin{align*}
P(Y|W) = \frac{P(W\cap Y)}{P(W)}
\end{align*}\]

&lt;p&gt;A.k.a.: “The probability that the Twins were playing the Yankees, given that I know they won last night, are the same as the number of times the Twins have played and beaten the Yankees divided by the number of playoff games the Twins have ever won.”&lt;/p&gt;

&lt;p&gt;If you’re perceptive you might notice that $P(W\cap Y)$ appears in both equations. We can solve for $P(W\cap Y)$ in one equation and stick it into the other, arriving at Bayes Theorem:&lt;/p&gt;

\[\begin{align*}
P(W|Y) = \frac{P(Y|W) P(W)}{P(Y)}
\end{align*}\]

&lt;p&gt;This is a slightly more confusing equation that reads “The probability that the Twins win their playoff game given that it is against the Yankees, is the same as the probability that the Twins played the Yankees given that we know they won that game, multiplied by the fraction of playoff games the Twins have ever won, divided by the fraction of total playoff games the Twins have played against the Yankees.”&lt;/p&gt;

&lt;p&gt;This sentence is confusing, but isn’t really necessary to understand in detail, because the point of this equation is to use it to do statistics. Scientists most frequently use this theorem in the context of understanding how the data they get from their experiments change their model/theory of the phenomenon they are investigating.&lt;/p&gt;

&lt;p&gt;For example, let’s say you are a physicist who is studying a new particle, and you want to measure its mass. Let’s say you already have some idea of what the mass is: you have a most likely value for the mass, and some idea of how good that estimate is in the first place. You can model this understanding as a normal distribution, centered at your best estimate of the mass, and with a standard deviation that quantifies your understanding of how good the estimate is.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;/assets/images/normal-distribution.png&quot; alt=&quot;Foo&quot; /&gt;

  &lt;figcaption&gt;Several examples of normal distributions. Figure author: Wikipedia user Inductiveload.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now let’s say you go make 15 measurements of the mass of this particle. Bayes Theorem gives you a way to take in those measurements, and use them to “update” your model of the particle’s mass. Maybe the mean of the distribution will change, and hopefully the standard deviation will decrease (i.e. your understanding improves). To do this, Bayes Theorem tells you to compute the “likelihood” of getting the measurements given your current model of the particle mass, and multiply it by the model itself (called the “prior” distrubution). Because the final thing you are getting (the “posterior” distribution) is still a probability distribution, you then have to normalize it to 1 (this is how I understand $P(Y)$ in this example). This procedure for taking a model of something, incorporating new data, and computing the updated model given the data, is extremely powerful.&lt;/p&gt;

&lt;h3 id=&quot;conjugate-priors&quot;&gt;Conjugate priors&lt;/h3&gt;

&lt;p&gt;In certain situations, multiplying the prior and likelihood distributions together to get the posterior distribution can, by stroke of fortuitous math, return you a distribution with the same form as the prior distribution. This means that the mathematics of propagating likelihood information through a Bayesian model is very straightforward, and even better, can be repeated without the form of the posterior becoming a total disaster. One of these conjugate prior-posterior relationships occurs when you are trying to estimate the mean of a normal distribution.&lt;/p&gt;

&lt;p&gt;We can return to the model from the previous section, where our physicist (you) is trying to measure the mass of a particle. The way physicists typically measure the mass of a particle is to watch it break up into other product particles, and then measure the total energy of all of the products. It turns out that, due to measurement error, and even quantum mechanics, the total energy of these products does not always add up to the exact same number, even if the starting particle is the same every time. This means that if we measure the total energy of the products a bunch of times and plot the distribution, we will get a normal distribution, where the mean represents the mass of the particle, and the standard deviation represents its decay rate, or the inverse of its half-life (i.e. a stable particle like an electron would appear ).&lt;/p&gt;

&lt;p&gt;If we take the Bayesian approach, and say that we do not &lt;em&gt;know&lt;/em&gt; what the mean is, but rather have a prior for it, choosing that prior distribution to also be a normal distribution will give us a conjugate prior-posterior relationship. Let’s work it out:&lt;/p&gt;

&lt;p&gt;The mass of our particle has some value $\mu$, and the width has some value $\Gamma$. The profile of the particle is then&lt;/p&gt;

\[\begin{align*}
P(x | \mu, \Gamma) = \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x-\mu)^2}{2\Gamma^2}
\end{align*}\]

&lt;p&gt;Our conjugate prior on $\mu$ has, in this case, the same form:&lt;/p&gt;

\[\begin{align*}
p_0(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}e^\frac{-(\mu-\mu_0)^2}{2\sigma_0^2}
\end{align*}\]

&lt;p&gt;What we need now is the likelihood, which is really similar to $P\left(x | \mu, \Gamma\right)$ but where you insert the values you have actually measured for $x$, which we’ll now call $\textbf{x} = {x_1, x_2, … x_n}$ for $n$ observations. Since we are computing the likelihood of measuring all of those observations, we will just multiply all of them together:&lt;/p&gt;

\[\begin{align*}
L = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x_i-\mu)^2}{2\Gamma^2} \\
L = \frac{1}{\left(2\pi\Gamma^2\right)^{\frac{n}{2}}} e^\frac{\sum_{i=1}^{n}-(x_i - \mu)^2}{2\Gamma^2} \\
\end{align*}\]

&lt;p&gt;After multiplying this by $p_0(\mu)$ and refactoring, we get:&lt;/p&gt;

\[\begin{align*}
p_1(\mu) = \frac{1}{\sqrt{2\pi\sigma_1^2}}e^\frac{-(\mu-\mu_1)^2}{2\sigma_1^2}
\end{align*}\]

&lt;p&gt;where $\sigma_1 = \left( \frac{1}{\sigma_0^2} + \frac{n}{\Gamma^2} \right)^{-1}$ and $\mu_1 = \sigma_1 \left( \frac{\mu_0}{\sigma_0^2}+\frac{\sum_{i=1}^n x_i}{\Gamma^2} \right)$. In the limiting case where $n \rightarrow \infty$, this reduces to $\sigma_1 \rightarrow \frac{\Gamma^2}{n} \rightarrow 0$, and $\mu_1 \rightarrow \frac{\Gamma^2}{n}\frac{\sum_{i=1}^n x_i}{\Gamma^2} \rightarrow \bar{\textbf{x}}$, meaning that our estimate of $\mu$ gets arbitrarily precise over time. You can do the exact same thing if you assume that $\mu$ is known but $\Gamma$ is not, in which case the conjugate prior is an inverse-gamma distribution. If you want to place a prior on both $\mu$ and $\Gamma$ simulaneously, you can do this with a normal-inverse-gamma distribution.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;$\cap$ is a math symbol that basically means “and”. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Micah Buuck</name></author><category term="Blog" /><category term="Statistics" /><category term="Machine Learning" /><summary type="html">Bayes’s Theorem I am an ancestral Minnesota Twins fan, having grown up in Minnesota during the glory days of the M&amp;amp;M boys, Johan Santana, and Ron Gardenhire. Back then, the Twins won a lot of games, just…not during the postseason. In fact, the Twins have never had much success in October, making the playoffs only 14 times out of 61 seasons. In that time, they’ve compiled a 25-44 record, which is not good, but given that they’re always playing the best teams in the playoffs, it’s not horrific. Unfortunately, most of those losses have come since the last time they won the World Series in 1991, and shockingly, 16 of them are to a single team: the New York Yankees, who they’ve beaten exactly twice in that time. This face was bad news for Twins fans. Photo by Christian Petersen // Getty Images So, let’s say you’re a Twins fan, and you’re excited that your team has made it to the playoffs, and you don’t yet know what team they’ll be facing, but it could be the Yankees. What is the probability that the Twins will win the first game they play? The most straightforward way to predict this is to assume that current trends will hold: they will probably lose (25/(25+44) = 36% chance of winning), but they are much more likely to lose if they are playing the Yankees (2/(2+16) = 11% chance of winning). Bayes’s theorem is a theorem invented by Thomas Bayes (shock) and refined and published by Richard Price that provides a formalism for this kind of conditional thinking. It starts with the idea of a “conditional probability”, or in our example, what is the probability that the Twins are going to win a playoff game given that we know they are playing the Yankees? \[\begin{align*} P(W|Y) = \frac{P(W\cap Y)}{P(Y)} \end{align*}\] That equation essentially reads “The probability ($P$) that the Twins will win ($W$) if they are playing the Yankees ($Y$) is the same as the fraction of playoff games where the Twins have played and beaten the Yankees ($P(W\cap Y)$)1 divided by the fraction of playoff games the Twins have played against the Yankees ($P(Y)$).” Let’s say you missed the game and didn’t even know who the Twins were playing, but you hear the next day that the Twins won. You might then ask yourself (if you are a nerd), “Given that I know the Twins won their game last night, what is the probability it was against the Yankees?” In that case the equation would be: \[\begin{align*} P(Y|W) = \frac{P(W\cap Y)}{P(W)} \end{align*}\] A.k.a.: “The probability that the Twins were playing the Yankees, given that I know they won last night, are the same as the number of times the Twins have played and beaten the Yankees divided by the number of playoff games the Twins have ever won.” If you’re perceptive you might notice that $P(W\cap Y)$ appears in both equations. We can solve for $P(W\cap Y)$ in one equation and stick it into the other, arriving at Bayes Theorem: \[\begin{align*} P(W|Y) = \frac{P(Y|W) P(W)}{P(Y)} \end{align*}\] This is a slightly more confusing equation that reads “The probability that the Twins win their playoff game given that it is against the Yankees, is the same as the probability that the Twins played the Yankees given that we know they won that game, multiplied by the fraction of playoff games the Twins have ever won, divided by the fraction of total playoff games the Twins have played against the Yankees.” This sentence is confusing, but isn’t really necessary to understand in detail, because the point of this equation is to use it to do statistics. Scientists most frequently use this theorem in the context of understanding how the data they get from their experiments change their model/theory of the phenomenon they are investigating. For example, let’s say you are a physicist who is studying a new particle, and you want to measure its mass. Let’s say you already have some idea of what the mass is: you have a most likely value for the mass, and some idea of how good that estimate is in the first place. You can model this understanding as a normal distribution, centered at your best estimate of the mass, and with a standard deviation that quantifies your understanding of how good the estimate is. Several examples of normal distributions. Figure author: Wikipedia user Inductiveload. Now let’s say you go make 15 measurements of the mass of this particle. Bayes Theorem gives you a way to take in those measurements, and use them to “update” your model of the particle’s mass. Maybe the mean of the distribution will change, and hopefully the standard deviation will decrease (i.e. your understanding improves). To do this, Bayes Theorem tells you to compute the “likelihood” of getting the measurements given your current model of the particle mass, and multiply it by the model itself (called the “prior” distrubution). Because the final thing you are getting (the “posterior” distribution) is still a probability distribution, you then have to normalize it to 1 (this is how I understand $P(Y)$ in this example). This procedure for taking a model of something, incorporating new data, and computing the updated model given the data, is extremely powerful. Conjugate priors In certain situations, multiplying the prior and likelihood distributions together to get the posterior distribution can, by stroke of fortuitous math, return you a distribution with the same form as the prior distribution. This means that the mathematics of propagating likelihood information through a Bayesian model is very straightforward, and even better, can be repeated without the form of the posterior becoming a total disaster. One of these conjugate prior-posterior relationships occurs when you are trying to estimate the mean of a normal distribution. We can return to the model from the previous section, where our physicist (you) is trying to measure the mass of a particle. The way physicists typically measure the mass of a particle is to watch it break up into other product particles, and then measure the total energy of all of the products. It turns out that, due to measurement error, and even quantum mechanics, the total energy of these products does not always add up to the exact same number, even if the starting particle is the same every time. This means that if we measure the total energy of the products a bunch of times and plot the distribution, we will get a normal distribution, where the mean represents the mass of the particle, and the standard deviation represents its decay rate, or the inverse of its half-life (i.e. a stable particle like an electron would appear ). If we take the Bayesian approach, and say that we do not know what the mean is, but rather have a prior for it, choosing that prior distribution to also be a normal distribution will give us a conjugate prior-posterior relationship. Let’s work it out: The mass of our particle has some value $\mu$, and the width has some value $\Gamma$. The profile of the particle is then \[\begin{align*} P(x | \mu, \Gamma) = \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x-\mu)^2}{2\Gamma^2} \end{align*}\] Our conjugate prior on $\mu$ has, in this case, the same form: \[\begin{align*} p_0(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}e^\frac{-(\mu-\mu_0)^2}{2\sigma_0^2} \end{align*}\] What we need now is the likelihood, which is really similar to $P\left(x | \mu, \Gamma\right)$ but where you insert the values you have actually measured for $x$, which we’ll now call $\textbf{x} = {x_1, x_2, … x_n}$ for $n$ observations. Since we are computing the likelihood of measuring all of those observations, we will just multiply all of them together: \[\begin{align*} L = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\Gamma^2}}e^\frac{-(x_i-\mu)^2}{2\Gamma^2} \\ L = \frac{1}{\left(2\pi\Gamma^2\right)^{\frac{n}{2}}} e^\frac{\sum_{i=1}^{n}-(x_i - \mu)^2}{2\Gamma^2} \\ \end{align*}\] After multiplying this by $p_0(\mu)$ and refactoring, we get: \[\begin{align*} p_1(\mu) = \frac{1}{\sqrt{2\pi\sigma_1^2}}e^\frac{-(\mu-\mu_1)^2}{2\sigma_1^2} \end{align*}\] where $\sigma_1 = \left( \frac{1}{\sigma_0^2} + \frac{n}{\Gamma^2} \right)^{-1}$ and $\mu_1 = \sigma_1 \left( \frac{\mu_0}{\sigma_0^2}+\frac{\sum_{i=1}^n x_i}{\Gamma^2} \right)$. In the limiting case where $n \rightarrow \infty$, this reduces to $\sigma_1 \rightarrow \frac{\Gamma^2}{n} \rightarrow 0$, and $\mu_1 \rightarrow \frac{\Gamma^2}{n}\frac{\sum_{i=1}^n x_i}{\Gamma^2} \rightarrow \bar{\textbf{x}}$, meaning that our estimate of $\mu$ gets arbitrarily precise over time. You can do the exact same thing if you assume that $\mu$ is known but $\Gamma$ is not, in which case the conjugate prior is an inverse-gamma distribution. If you want to place a prior on both $\mu$ and $\Gamma$ simulaneously, you can do this with a normal-inverse-gamma distribution. $\cap$ is a math symbol that basically means “and”. &amp;#8617;</summary></entry></feed>