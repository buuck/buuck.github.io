<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Climate Impacts of Scientific Computing in Particle Physics - Micah Buuck, PhD</title>
<meta name="description" content="Snowmass 2021 From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022.  As a part of this process, I worked on a white paper that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share.  Climate Impacts of Scientific Computing in Particle Physics  High-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by Lannelongue et al.  The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective.  Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website green-algorithms.org can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available.  Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the Green500, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), 80.3% of electricity consumption in Florida in 2019 was fossil fueled while only 35.9% California’s electricity consumption over the same period was fossil fueled, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.1  While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions.  Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. Ahmed et al. have completed a study of this possibility.                 This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. &#8617;">


  <meta name="author" content="Micah Buuck">
  
  <meta property="article:author" content="Micah Buuck">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Micah Buuck, PhD">
<meta property="og:title" content="Climate Impacts of Scientific Computing in Particle Physics">
<meta property="og:url" content="http://localhost:4000/blog/computing-and-climate/">


  <meta property="og:description" content="Snowmass 2021 From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022.  As a part of this process, I worked on a white paper that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share.  Climate Impacts of Scientific Computing in Particle Physics  High-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by Lannelongue et al.  The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective.  Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website green-algorithms.org can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available.  Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the Green500, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), 80.3% of electricity consumption in Florida in 2019 was fossil fueled while only 35.9% California’s electricity consumption over the same period was fossil fueled, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.1  While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions.  Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. Ahmed et al. have completed a study of this possibility.                 This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. &#8617;">







  <meta property="article:published_time" content="2022-12-06T00:00:00-08:00">






<link rel="canonical" href="http://localhost:4000/blog/computing-and-climate/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Micah Buuck, PhD Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



<!-- for mathjax support -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
    });
  </script>
  <script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Micah Buuck, PhD
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/headshot.jpg" alt="Micah Buuck" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Micah Buuck</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>I use liquid xenon to hunt for dark matter with the LZ experiment. I also have an interest in all things related to data science and decarbonization.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://twitter.com/buucky_wuucky" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/buuck" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Climate Impacts of Scientific Computing in Particle Physics">
    <meta itemprop="description" content="Snowmass 2021From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022.As a part of this process, I worked on a white paper that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share.Climate Impacts of Scientific Computing in Particle PhysicsHigh-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by Lannelongue et al.The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective.Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website green-algorithms.org can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available.Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the Green500, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), 80.3% of electricity consumption in Florida in 2019 was fossil fueled while only 35.9% California’s electricity consumption over the same period was fossil fueled, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.1While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions.Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. Ahmed et al. have completed a study of this possibility.            This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. &#8617;      ">
    <meta itemprop="datePublished" content="2022-12-06T00:00:00-08:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Climate Impacts of Scientific Computing in Particle Physics
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3 id="snowmass-2021">Snowmass 2021</h3>
<p>From 2020 to 2022, the US particle physics community collectively worked through a community planning process, which is called Snowmass for historical reasons. This consisted of many small groups working together to write white papers on various topics, such as what kind of experiments should be built in the next 10 years, what the technological needs of the field are, and how to improve inclusion, diversity, equity, and accessibility in the field. The process culminated in a 10 day meeting at the University of Washington in Seattle (in years past this was at Snowmass, hence the name), where all of the white papers were distilled into a series of reports, which will themselves be distilled by a committee of community leaders and experts into a single report that will be shared with the US Congress and Department of Energy. This process has its own website, where you can find all the information you could possibly ever want on the state of US particle physics in 2022.</p>

<p>As a part of this process, I worked on a <a href="https://arxiv.org/abs/2203.12389" target="_blank">white paper</a> that assessed the contributions of particle physics research to climate change, in particular looking at places where greenhouse gasses are directly or indirectly emitted. I authored a section on the ways in which scientific computing creates GHG emissions, which I’ll now share.</p>

<h2 id="climate-impacts-of-scientific-computing-in-particle-physics">Climate Impacts of Scientific Computing in Particle Physics</h2>

<p>High-performance computing (HPC) is an essential part of physics research. It is also a growing source of greenhouse gas emissions, primarily due to the large amount of electricity used by computation itself. Across all sectors, data centers and computing already contribute approximately 2-4% of global GHG emissions, and that fraction is only predicted to grow in the next 10 years. While the environmental costs of HPC are not unknown, they are often not prioritized, or even discussed, when planning and scheduling computational projects. We believe the particle physics community should improve its accounting of this issue. We provide several specific suggestions below, many of which are inspired by <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009324" target="_blank">Lannelongue et al.</a></p>

<p>The best way to reduce the carbon emissions of high-performance computing is to ensure that compute facilities are powered by carbon-free energy sources. In the early planning stages of a compute facility, when siting decisions are being made, the carbon intensity of the electricity used to power the facility should be a major factor into the decision making process. In many places, renewable electricity is already cheaper than fossil-fueled electricity, making this a cost-effective decision as well. Local installation of renewable energy generating capacity can also reduce the carbon impact of both future and existing computing facilities, most easily with rooftop solar panels. Other options may also be available depending on the location of the compute center. These strategies have a direct and immediate impact on the carbon emissions associated with a compute center, and do not rely on the cooperation of users as several of our other suggestions do, making them likely more effective.</p>

<p>Reductions in GHG emissions can also be achieved on the demand side with current technology, through a combination of optimization of resource use and careful planning of timing and siting of computation. These suggestions give individual users tools to directly reduce their own carbon emissions impact. However, it is often not obvious how to successfully follow them, since the information required is unavailable or difficult to access. The website <a href="https://www.green-algorithms.org" target="_blank">green-algorithms.org</a> can provide a useful estimate of the cost of running a particular algorithm, but it requires knowledge of the specific hardware a computation will be using, and also the specific energy mix the compute center uses, to calculate the most accurate emissions estimate. This information could be provided by compute centers, and while the former frequently is, the latter often is not. If they aren’t already doing so, compute centers should provide information on the GHG intensity of their computation, ideally in a standardized format to facilitate easy comparisons. This information could be integrated into the green-algorithms website, or else made available in a standalone website, and statistics from cloud computing services could also be included if that information is available.</p>

<p>Going a step further, compute centers, and the scientific collaborations that make use of them, could even provide a simple tool to estimate the GHG emissions impact of a particular job request, perhaps as a simple command-line utility. A precise estimate would be difficult to achieve, but with knowledge of the hardware, energy mix, and particulars of the job request, a reasonably accurate range of possible GHG emissions should be possible to produce, likely ranging from zero to full resource utilization. Compute centers could then report their CO2e intensity to a common location, ideally a centralized website of some sort. A version of this already exists as the <a href="https://www.top500.org/lists/green500/" target="_blank">Green500</a>, an offshoot of the better-known TOP500. However, the Green500 compares the power efficiency of various systems, which is related to, but not the same as, the CO2e intensity. For example, while HiPerGator AI at the University of Florida has a slightly better power efficiency rating than Perlmutter at the National Energy Research Supercomputing Center (NERSC, Berkeley, CA), <a href="https://www.eia.gov/beta/states/states/fl/overview" target="_blank">80.3% of electricity consumption in Florida in 2019 was fossil fueled</a> while <a href="https://www.eia.gov/beta/states/states/ca/overview" target="_blank">only 35.9% California’s electricity consumption over the same period was fossil fueled</a>, meaning that the per-FLOP CO2e emissions impact of Perlmutter is likely less than that of HiPerGator AI.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>While obtaining better information about compute center GHG emissions can be very useful for long-term planning, such as an experiment deciding where to do the bulk of their computing, it is less useful for individual users who often do not have the ability to choose where to do their large-scale computing. Users do usually have the ability to improve the efficiency of their code, which can lead to substantial improvements in emissions if done properly. However, frequently users are writing code that makes use of large libraries, such as Geant4 or ROOT, that they do not have control over, which can limit potential improvements from optimization. Developers of these libraries should be sure to provide information on how to minimize GHG emissions, such as optimal hardware, and scaling of memory utilization. Tracking the GHG emissions efficiency over software releases with some kind of standard benchmark would provide an incentive to minimize emissions.</p>

<p>Even if one makes optimal choices to minimize the GHG emissions of their computation, some emissions will still inherently result from the electricity used. Even if the electricity is produced from renewable sources, the life cycle emissions of those power sources, and also of the compute center itself, are nonzero. Purchasing carbon offsets, while never a true substitute for emissions reductions, can help to mitigate the remaining unavoidable emissions. Incentivizing the purchasing of carbon offsets for unavoidable emissions is something funding agencies could do. Some Department of Energy compute centers, such as NERSC, use an allocation-and-charging framework for distributing computational resources. Carbon offsets could be folded into the cost of batch jobs by the compute center, drawn directly from the user’s allocation. This would incentivize experiments to choose greener compute centers, and users to ensure that their code runs as efficiently as possible. At minimum, compute centers could provide links to legitimate offset companies, or other agencies that rate and verify them. Finally, compute centers could coordinate their power loads with their electricity suppliers, scheduling more jobs when electricity is cheap and relatively clean (e.g. midday or nighttime), and reducing their loads when electricity is expensive and relatively dirty (e.g. late afternoon through the early evening). For the greatest impact, this would require real-time information from the electricity supplier, but if the compute center is a sufficiently large electricity customer, the supplier would likely be happy to provide this information since it would improve their demand response. <a href="https://ieeexplore.ieee.org/abstract/document/8622871" target="_blank">Ahmed et al.</a> have completed a study of this possibility.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is only an example to illustrate the concept; obviously the exact energy mix for each compute center is likely not identical to the statewide average. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#climate-change" class="page__taxonomy-item" rel="tag">Climate Change</a><span class="sep">, </span>
    
      <a href="/tags/#computing" class="page__taxonomy-item" rel="tag">Computing</a><span class="sep">, </span>
    
      <a href="/tags/#physics" class="page__taxonomy-item" rel="tag">Physics</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#blog" class="page__taxonomy-item" rel="tag">Blog</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2022-12-06T00:00:00-08:00">December 6, 2022</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Climate+Impacts+of+Scientific+Computing+in+Particle+Physics%20http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcomputing-and-climate%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcomputing-and-climate%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcomputing-and-climate%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/blog/predictive-uncertainty-gammatpc/" class="pagination--pager" title="Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept
">Previous</a>
    
    
      <a href="/CAISO-Analysis/" class="pagination--pager" title="Caiso Analysis
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/world-electricity-and-air-pollution/" rel="permalink">World Electricity And Air Pollution
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Connecting world electricity production with air pollution

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/CAISO-Analysis/" rel="permalink">Caiso Analysis
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Analysis of 2021-2022 CAISO Power Source Data

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/predictive-uncertainty-gammatpc/" rel="permalink">Predictive Uncertainty with Neural Networks for the GammaTPC Telescope Concept
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Uncertainty in Physics

Experimental physicists spend most of their research time doing 2 things: building their experiment, and trying to figure out how wro...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/bayes-theorem/" rel="permalink">Basic Bayesian Statistics
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Bayes’s Theorem

I am an ancestral Minnesota Twins fan, having grown up in Minnesota during the glory days of the M&amp;M boys, Johan Santana, and Ron Garden...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/buucky_wuucky" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/buuck" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Micah Buuck, PhD. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
